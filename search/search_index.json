{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Distributional Refinement Network Python Package","text":"<p>A PyTorch-based library for Distributional Refinement Networks and distributional regression modeling, combining interpretable baseline models with flexible neural networks for advanced distributional forecasting in actuarial and statistical contexts.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from drn import GLM, DRN\nimport pandas as pd\n\n# Load your data\nDATA_URL = \"https://raw.githubusercontent.com/agi-lab/DRN/refs/heads/main/data/processed/synth/\"\nX_train = pd.read_csv(DATA_URL + \"x_train.csv\")\ny_train = pd.read_csv(DATA_URL + \"y_train.csv\")\n\n# Train a baseline GLM\nglm_model = GLM(\"gamma\").fit(X_train, y_train)\n\n# Create and train a DRN\ndrn_model = DRN(glm_model).fit(X_train, y_train)\n\n# Make distributional predictions\npredictions = drn_model.predict(X_test)\nmean_pred = predictions.mean\nquantiles = predictions.quantiles([10, 50, 90])\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<p>Comprehensive Model Suite: The package includes Distributional Refinement Networks (DRN) as the main model, alongside Combined Actuarial Neural Networks (CANN), Mixture Density Networks (MDN), Deep Distribution Regression (DDR), and interpretable Generalized Linear Models (GLM).</p> <p>Distributional Flexibility: Rather than predicting single point estimates, these models forecast entire distributions with user-controlled refinement ranges. The framework supports bounded, unbounded, discrete, continuous, or mixed response variables through tailored regularization including KL divergence, roughness penalties, and mean constraints.</p> <p>Complete Evaluation Framework: Models provide full distributional information including density functions, CDFs, means, and quantiles. Evaluation uses both traditional metrics (RMSE) and distributional measures (CRPS, Quantile Loss, NLL).</p> <p>Interpretability: Integrated Kernel SHAP analysis decomposes baseline and neural contributions, providing understanding of model adjustments beyond simple mean predictions.</p>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TD\n    A[Raw Data] --&gt; B[Data Preprocessing]\n    B --&gt; C[Baseline Model Training]\n    C --&gt; D[DRN Refinement]\n    D --&gt; E[Distributional Predictions]\n    E --&gt; F[Evaluation &amp; Interpretability]\n\n    C --&gt; G[GLM/Other Models]\n    D --&gt; H[Neural Network Refinement]\n    E --&gt; I[PDF, CDF, Quantiles]\n    F --&gt; J[SHAP Analysis]</code></pre> <p>The package centers around several model classes inheriting from BaseModel, including the main DRN implementation, GLM baselines with Gaussian and Gamma distributions, and advanced neural models (CANN, MDN, DDR). Supporting utilities provide PyTorch-based training with early stopping, comprehensive evaluation metrics, SHAP-based interpretability, and data preprocessing.</p>"},{"location":"#the-drn-approach","title":"The DRN Approach","text":"<p>Distributional Refinement Networks address three key challenges in actuarial modeling: enabling flexible covariate impact across different aspects of the conditional distribution, integrating machine learning advances while maintaining interpretability, and preserving model transparency for trusted decision-making.</p> <p>The approach starts with an interpretable baseline model (typically a GLM), then applies neural network refinements to the entire distribution. Regularization balances flexibility with interpretability, while SHAP analysis provides comprehensive distributional explanations.</p>"},{"location":"#related-work","title":"Related Work","text":"<p>This package accompanies the DRN paper on Distributional Refinement Networks. For reproducible research and additional experiments, visit our research repository.</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<p>The documentation covers installation and basic setup in Getting Started, complete technical specifications in the API Reference.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use DRN in your research, please cite:</p> <pre><code>@misc{avanzi2024distributional,\n    title={Distributional Refinement Network: Distributional Forecasting via Deep Learning}, \n    author={Benjamin Avanzi and Eric Dong and Patrick J. Laub and Bernard Wong},\n    year={2024},\n    eprint={2406.00998},\n    archivePrefix={arXiv},\n    primaryClass={stat.ML}\n}\n</code></pre>"},{"location":"#contact","title":"Contact","text":"<p>For questions or support, contact tiandong1999@gmail.com.</p>"},{"location":"api/","title":"API Reference","text":"<p>This section provides comprehensive documentation for all DRN classes, functions, and modules. The API documentation is automatically generated from the source code docstrings, similar to R package documentation.</p>"},{"location":"api/#overview","title":"Overview","text":"<p>The DRN package is organized into several key modules:</p> <ul> <li>Models - Core distributional regression models (GLM, DRN, CANN, MDN, DDR)</li> <li>Distributions - Custom distribution implementations and utilities  </li> <li>Training - Training functions, loss functions, and optimization utilities</li> <li>Metrics - Evaluation metrics for distributional forecasting</li> <li>Interpretability - Model explanation and visualization tools</li> <li>Utilities - Data preprocessing, splitting, and helper functions</li> </ul>"},{"location":"api/#quick-reference","title":"Quick Reference","text":""},{"location":"api/#core-classes","title":"Core Classes","text":"Class Purpose Key Methods <code>BaseModel</code> Abstract base for all models <code>.fit()</code>, <code>.predict()</code>, <code>.quantiles()</code> <code>GLM</code> Generalized Linear Models <code>.fit()</code>, <code>.predict()</code>, <code>.clone()</code> <code>DRN</code> Distributional Refinement Network <code>.fit()</code>, <code>.predict()</code>, <code>.log_adjustments()</code>"},{"location":"api/#key-functions","title":"Key Functions","text":"Function Purpose Module <code>train()</code> Train models with PyTorch <code>drn.train</code> <code>drn_loss()</code> DRN loss function <code>drn.models</code> <code>crps()</code> Continuous Ranked Probability Score <code>drn.metrics</code> <code>split_and_preprocess()</code> Data preprocessing <code>drn.utils</code>"},{"location":"api/distributions/","title":"Distributions","text":"<p>Custom distribution implementations and utilities for distributional regression. These extend PyTorch distributions with specialized functionality for DRN models.</p>"},{"location":"api/distributions/#histogram-distributions","title":"Histogram Distributions","text":""},{"location":"api/distributions/#histogram","title":"Histogram","text":""},{"location":"api/distributions/#drn.distributions.histogram.Histogram","title":"<code>drn.distributions.histogram.Histogram</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>This class represents a histogram distribution. Basically, the distribution is a composite of uniform distributions over the bins.</p>"},{"location":"api/distributions/#drn.distributions.histogram.Histogram-attributes","title":"Attributes","text":""},{"location":"api/distributions/#drn.distributions.histogram.Histogram.mean","title":"<code>mean</code>  <code>property</code>","text":"<p>Calculate the mean of the distribution. Returns:     the mean (shape: (batch_shape,))</p>"},{"location":"api/distributions/#drn.distributions.histogram.Histogram-functions","title":"Functions","text":""},{"location":"api/distributions/#drn.distributions.histogram.Histogram.__init__","title":"<code>__init__(cutpoints, prob_masses)</code>","text":"<p>Args:     regions: the bin boundaries (shape: (K+1,))     prob_masses: the probability for landing in each regions (shape: (n, K))</p>"},{"location":"api/distributions/#drn.distributions.histogram.Histogram.cdf","title":"<code>cdf(value)</code>","text":"<p>Calculate the cumulative distribution function for the given values.</p>"},{"location":"api/distributions/#drn.distributions.histogram.Histogram.cdf_at_cutpoints","title":"<code>cdf_at_cutpoints()</code>","text":"<p>Calculate the cumulative distribution function at each cutpoint.</p>"},{"location":"api/distributions/#drn.distributions.histogram.Histogram.cdf_same_eval","title":"<code>cdf_same_eval(value)</code>","text":"<p>Calculate the cumulative distribution function for the same value across the batch.</p>"},{"location":"api/distributions/#drn.distributions.histogram.Histogram.icdf","title":"<code>icdf(p, l=None, u=None, max_iter=1000, tolerance=1e-07)</code>","text":"<p>Calculate the inverse CDF (quantiles) using shared binary search implementation.</p>"},{"location":"api/distributions/#drn.distributions.histogram.Histogram.log_prob","title":"<code>log_prob(value)</code>","text":"<p>Calculate the log probability densities of <code>values</code>.</p>"},{"location":"api/distributions/#drn.distributions.histogram.Histogram.prob","title":"<code>prob(value)</code>","text":"<p>Calculate the probability densities of <code>values</code>.</p>"},{"location":"api/distributions/#drn.distributions.histogram.Histogram.quantiles","title":"<code>quantiles(percentiles, l=None, u=None, max_iter=1000, tolerance=1e-07)</code>","text":"<p>Calculate the quantile values for the given percentiles (cumulative probabilities * 100).</p>"},{"location":"api/distributions/#extended-histogram","title":"Extended Histogram","text":""},{"location":"api/distributions/#drn.distributions.extended_histogram.ExtendedHistogram","title":"<code>drn.distributions.extended_histogram.ExtendedHistogram</code>","text":"<p>               Bases: <code>Distribution</code></p> <p>This class represents a splicing of a supplied distribution with a histogram distribution. The histogram part is defined by K regions with boundaries -infty &lt; c_0 &lt; c_1 &lt; ... &lt; c_K &lt; infty. The final density before c_0 &amp; after c_K is the same as the original distribution. The density between c_k &amp; c_{k+1} is defined by the histogram distribution.</p>"},{"location":"api/distributions/#drn.distributions.extended_histogram.ExtendedHistogram-attributes","title":"Attributes","text":""},{"location":"api/distributions/#drn.distributions.extended_histogram.ExtendedHistogram.mean","title":"<code>mean</code>  <code>property</code>","text":"<p>Calculate the mean of the distribution. Returns:     the mean (shape: (batch_shape,))</p>"},{"location":"api/distributions/#drn.distributions.extended_histogram.ExtendedHistogram-functions","title":"Functions","text":""},{"location":"api/distributions/#drn.distributions.extended_histogram.ExtendedHistogram.__init__","title":"<code>__init__(baseline, cutpoints, pmf, baseline_probs=None)</code>","text":"<p>Args:     baseline: the original distribution     cutpoints: the bin boundaries (shape: (K+1,))     pmf: the refined (cond.) probability for landing in each region (shape: (n, K))     baseline_probs: the baseline's probability for landing in each region (shape: (n, K))</p>"},{"location":"api/distributions/#drn.distributions.extended_histogram.ExtendedHistogram.baseline_prob_between_cutpoints","title":"<code>baseline_prob_between_cutpoints()</code>","text":"<p>Calculate the baseline probability vector</p>"},{"location":"api/distributions/#drn.distributions.extended_histogram.ExtendedHistogram.cdf","title":"<code>cdf(value)</code>","text":"<p>Calculate the cumulative distribution function for the given values.</p>"},{"location":"api/distributions/#drn.distributions.extended_histogram.ExtendedHistogram.cdf_at_cutpoints","title":"<code>cdf_at_cutpoints()</code>","text":"<p>Calculate the cumulative distribution function at each cutpoint.</p>"},{"location":"api/distributions/#drn.distributions.extended_histogram.ExtendedHistogram.icdf","title":"<code>icdf(p, l=None, u=None, max_iter=1000, tolerance=1e-07)</code>","text":"<p>Calculate the inverse CDF (quantiles) using shared binary search implementation.</p>"},{"location":"api/distributions/#drn.distributions.extended_histogram.ExtendedHistogram.prob","title":"<code>prob(value)</code>","text":"<p>Calculate the probability densities of <code>values</code>.</p>"},{"location":"api/distributions/#drn.distributions.extended_histogram.ExtendedHistogram.quantiles","title":"<code>quantiles(percentiles, l=None, u=None, max_iter=1000, tolerance=1e-07)</code>","text":"<p>Calculate the quantile values for the given percentiles (cumulative probabilities * 100).</p>"},{"location":"api/distributions/#drn.distributions.extended_histogram.ExtendedHistogram.real_adjustments","title":"<code>real_adjustments()</code>","text":"<p>Calculate the real adjustment factors a_k's</p>"},{"location":"api/distributions/#specialized-distributions","title":"Specialized Distributions","text":""},{"location":"api/distributions/#inverse-gaussian","title":"Inverse Gaussian","text":""},{"location":"api/distributions/#drn.distributions.inverse_gaussian.InverseGaussian","title":"<code>drn.distributions.inverse_gaussian.InverseGaussian</code>","text":"<p>               Bases: <code>Distribution</code></p>"},{"location":"api/distributions/#parameter-estimation","title":"Parameter Estimation","text":""},{"location":"api/distributions/#gamma-parameter-conversion","title":"Gamma Parameter Conversion","text":""},{"location":"api/distributions/#drn.distributions.estimation.gamma_convert_parameters","title":"<code>drn.distributions.estimation.gamma_convert_parameters(mu, phi)</code>","text":"<p>Our models predict the mean of the gamma distribution, but we need the shape and rate parameters. This function converts the mean and dispersion parameter into the shape and rate parameters. Args:     mu: the predicted means for the gamma distributions (shape: (n,))     phi: the dispersion parameter Returns:     alpha: the shape parameter (shape: (n,))     beta: the rate parameter (shape: (n,))</p>"},{"location":"api/distributions/#dispersion-estimation","title":"Dispersion Estimation","text":""},{"location":"api/distributions/#drn.distributions.estimation.estimate_dispersion","title":"<code>drn.distributions.estimation.estimate_dispersion(distribution, mu, y, p)</code>","text":"<p>Estimate the dispersion parameter for different distributions.</p> <p>Parameters: distribution (str): The type of distribution (\"gamma\", \"gaussian\", \"inversegaussian\", \"lognormal\"). mu (torch.Tensor): The predicted mean values. y (torch.Tensor): The observed target values. p (int): The number of model parameters.</p> <p>Returns: torch.Tensor: The estimated dispersion parameter.</p>"},{"location":"api/interpretability/","title":"Interpretability","text":"<p>Model interpretation and explanation tools for understanding DRN predictions and distributional properties.</p>"},{"location":"api/interpretability/#drn-explainer","title":"DRN Explainer","text":"<p>Main class for interpreting DRN models using SHAP and custom visualization methods.</p>"},{"location":"api/interpretability/#drn.interpretability.DRNExplainer","title":"<code>drn.interpretability.DRNExplainer</code>","text":""},{"location":"api/interpretability/#drn.interpretability.DRNExplainer-functions","title":"Functions","text":""},{"location":"api/interpretability/#drn.interpretability.DRNExplainer.__init__","title":"<code>__init__(drn, glm, default_cutpoints, background_data_raw, preprocessor=None)</code>","text":"<p>Initialise the DRNExplainer with the given parameters.</p> <p>Args:     drn (torch.nn.Module): The DRN neural network model.     glm (torch.nn.Module): The baseline Generalised Linear Model (GLM).     default_cutpoints (list): Cutpoints used for training the DRN.     background_data_raw (pd.DataFrame): Background data prior to preprocessing,     preprocessor (ColumnTransformer, optional): To convert raw data into a format suitable for the DRN.</p>"},{"location":"api/interpretability/#drn.interpretability.DRNExplainer.cdf_plot","title":"<code>cdf_plot(instance, grid=None, cutpoints=None, other_df_models=None, model_names=None, synthetic_data=None, x_range=None, plot_title=None, plot_baseline=True, density_transparency=1.0, dist_property='Mean', quantile_bounds=None, nsamples_background_fraction=0.1, adjustment=True, method='Kernel', labelling_gap=0.01, top_K_features=3, y_range=None, shap_fontsize=25, figsize=None)</code>","text":"<p>Plot the cumulative distribution function.</p>"},{"location":"api/interpretability/#drn.interpretability.DRNExplainer.empirical_cdf","title":"<code>empirical_cdf(samples, x)</code>","text":"<p>Compute the empirical CDF for a given value x based on the provided samples.</p>"},{"location":"api/interpretability/#drn.interpretability.DRNExplainer.kernel_shap","title":"<code>kernel_shap(explaining_data, distributional_property, adjustment=True, nsamples_background_fraction=1.0, glm_output=False, other_shap_values=None)</code>","text":"<p>Pass on the explaining instance, background data, feature processing and value function to the KernelSHAP_DRN class</p>"},{"location":"api/interpretability/#drn.interpretability.DRNExplainer.kernel_shap_plot","title":"<code>kernel_shap_plot(instance_raw, instance, dist_property, quantile_bounds, method='Kernel', nsamples_background_fraction=1.0, adjustment=True, axes=None, top_K_features=3, y_max=None, y_min=None, labelling_gap=0.05, fontsize=25)</code>","text":"<p>Visualises the impact of SHAP values for the top K features on a specified distributional property, including the option to display adjustment effects.</p> <p>Parameters: - instance_raw (pd.DataFrame): The instance data prior to any processing. - instance (torch.Tensor): Processed instance data ready for the model. - dist_property (str): Target distributional property (e.g., 'Mean', 'Variance'). - method: Method used for SHAP value computation, defaulting to 'Kernel'. - nsamples_background_fraction (float): Fraction of background data utilised, defaults to 1.0. - adjustment (bool): Whether to include adjustment effects in the visualisation, defaults to True. - axes: Matplotlib axes object for plotting. - top_K_features (int): Number of top features to highlight based on SHAP values. - Plot styling parameters like <code>y_max</code>, <code>y_min</code>, <code>labelling_gap</code>, <code>fontsize</code> are for visual adjustments.</p>"},{"location":"api/interpretability/#drn.interpretability.DRNExplainer.max_pdf_in_region","title":"<code>max_pdf_in_region(drn_pdf, glm_pdf, interval_width, cutpoint_idx)</code>","text":"<p>Find the maximum pdf value within the region</p>"},{"location":"api/interpretability/#drn.interpretability.DRNExplainer.mean_drn","title":"<code>mean_drn(instances)</code>","text":"<p>Calculate the mean predicted by the DRN network given the selected instances/features</p>"},{"location":"api/interpretability/#drn.interpretability.DRNExplainer.mean_glm","title":"<code>mean_glm(instances)</code>","text":"<p>Calculate the mean predicted by the GLM given the selected instances/features</p>"},{"location":"api/interpretability/#drn.interpretability.DRNExplainer.mean_value_function","title":"<code>mean_value_function(instances, adjustment)</code>","text":"<p>Calculate the mean value function given the selected instances/features</p>"},{"location":"api/interpretability/#drn.interpretability.DRNExplainer.plot_adjustment_factors","title":"<code>plot_adjustment_factors(instance, observation=None, cutpoints=None, num_interpolations=None, other_df_models=None, model_names=None, percentiles=None, cutpoints_label_bool=False, synthetic_data=None, plot_adjustments_labels=True, axes=None, x_range=None, y_range=None, plot_title=None, plot_mean_adjustment=False, plot_y_label=None, density_transparency=1.0, figsize=None)</code>","text":"<p>Plot the adjustment factors for each of the partitioned interval. expand: interpolation of cutpoints for density evaluations.</p>"},{"location":"api/interpretability/#drn.interpretability.DRNExplainer.plot_dp_adjustment_shap","title":"<code>plot_dp_adjustment_shap(instance_raw, dist_property='Mean', quantile_bounds=None, method='Kernel', nsamples_background_fraction=1.0, top_K_features=3, adjustment=True, other_df_models=None, model_names=None, cutpoints=None, num_interpolations=None, labelling_gap=0.05, synthetic_data=None, synthetic_data_samples=int(1000000.0), observation=None, plot_baseline=True, x_range=None, y_range=None, plot_y_label=None, plot_title=None, figsize=None, density_transparency=1.0, shap_fontsize=25, legend_loc='upper left')</code>","text":"<p>Plot SHAP value-based adjustments with an option to include density functions.</p> <p>Args:     instance_raw: Raw data before one-hot encoding, used for instance-specific analysis.     dist_property: Distributional property to adjust ('Mean', 'Variance', 'Quantile').     method: Technique for SHAP value computation ('Kernel', 'Tree', 'FastSHAP', etc.).     nsamples_background_fraction: Fraction of background data for SHAP calculation.     top_K_features: Number of top features based on SHAP values.     adjustment: Whether to plot the SHAP values of the adjusted or unadjusted distributional property     other_df_models: Other distributional forecasting models for comparison.     model_names: Names of the other distributional forecasting models.     cutpoints: Cutpoints for partitioning feature space, defaults to <code>self.default_cutpoints</code>.     num_interpolations: Number of points for density interpolation, defaults to 2000.     labelling_gap: Gap between labels in the plot for readability.     synthetic_data, synthetic_data_samples: Synthetic data function for true density comparison and number of samples generated.     observation: Specific observation value for vertical line plotting.     plot_baseline: Flag to include baseline model's density plot.     x_range, y_range: Axis ranges for the plot.     plot_y_label, plot_title: Custom labels for the plot's axes and title.     density_transparency: Alpha value for density plot transparency.     shap_fontsize, figsize, label_adjustment_factor: Plot styling parameters.     legend_loc: Location of the legend in the plot.</p>"},{"location":"api/interpretability/#drn.interpretability.DRNExplainer.quantile_drn","title":"<code>quantile_drn(instances, percentile=[90], grid=None)</code>","text":"<p>Calculate the quantile predicted by the DRN network given the selected instances/features</p>"},{"location":"api/interpretability/#drn.interpretability.DRNExplainer.quantile_glm","title":"<code>quantile_glm(instances, percentile=[90], grid=None)</code>","text":"<p>Calculate the quantile predicted by the GLM given the selected instances/features</p>"},{"location":"api/interpretability/#drn.interpretability.DRNExplainer.quantile_value_function","title":"<code>quantile_value_function(instances, adjustment, grid=None, percentile=[90])</code>","text":"<p>Calculate the quantile value function given the selected instances/features</p>"},{"location":"api/interpretability/#drn.interpretability.DRNExplainer.real_adjustment_factors","title":"<code>real_adjustment_factors(instances, cutpoints)</code>","text":"<p>Calculate the real adjustment factors.</p>"},{"location":"api/interpretability/#drn.interpretability.DRNExplainer.region_adjustments","title":"<code>region_adjustments(instance, region_start, region_end)</code>","text":"<p>Calculate and round the adjustment factors</p>"},{"location":"api/interpretability/#drn.interpretability.DRNExplainer.region_text","title":"<code>region_text(instance, interval_width, drn_pdf, glm_pdf, y_max, region_start, region_end, cutpoint_idx, adjustment_idx, cutpoints_label_bool=False, percentiles=None)</code>","text":"<p>Text the density adjustment regions</p>"},{"location":"api/interpretability/#drn.interpretability.DRNExplainer.set_value_function","title":"<code>set_value_function(distributional_property, adjustment, model_function)</code>","text":"<p>Calculate the numeric part from the distributional property XX% quantile. Set the value function accordingly.</p>"},{"location":"api/interpretability/#kernel-shap-integration","title":"Kernel SHAP Integration","text":"<p>Specialized SHAP explainer for distributional properties of DRN models.</p>"},{"location":"api/interpretability/#drn.kernel_shap_explainer.KernelSHAP_DRN","title":"<code>drn.kernel_shap_explainer.KernelSHAP_DRN</code>","text":"<p>This class produces the Kernel SHAP values regarding the distributional property of interest. It produces the raw Kernel SHAP values. It also generates SHAP dependence plot for any pair of features, considering categorical features. Beeswarm plot can be generated for any features.</p>"},{"location":"api/interpretability/#drn.kernel_shap_explainer.KernelSHAP_DRN-functions","title":"Functions","text":""},{"location":"api/interpretability/#drn.kernel_shap_explainer.KernelSHAP_DRN.__init__","title":"<code>__init__(explaining_data, nsamples_background_fraction, background_data_raw, value_function, glm_value_function, other_shap_values=None, random_state=42)</code>","text":"<p>Args: See the DRNExplainer class for explanations regarding {explaining_data, nsamples_background_fraction, background_data_raw, preprocessor} value_function: v_{M}(S, x), given any instance x and indices S \\subseteq {1, ..., p}</p>"},{"location":"api/interpretability/#drn.kernel_shap_explainer.KernelSHAP_DRN.beeswarm_plot","title":"<code>beeswarm_plot(features=None, output='value')</code>","text":"<p>Create the beeswarm summary plots features: a list of feature names required for plotting adjusting: False --&gt; explaining the drn model; True --&gt; explaining how the drn adjusts the glm</p>"},{"location":"api/interpretability/#drn.kernel_shap_explainer.KernelSHAP_DRN.forward","title":"<code>forward()</code>","text":"<p>The raw Kernel SHAP (either adjusted or DRN) output.</p>"},{"location":"api/interpretability/#drn.kernel_shap_explainer.KernelSHAP_DRN.global_importance_plot","title":"<code>global_importance_plot(features=None, output='value')</code>","text":"<p>Creates a global importance plot based on the absolute SHAP values.</p>"},{"location":"api/interpretability/#drn.kernel_shap_explainer.KernelSHAP_DRN.shap_dependence_plot","title":"<code>shap_dependence_plot(features_tuple, output='value')</code>","text":"<p>Create the SHAP dependence plots features_tuple: the pair of features required for plotting other_shap_values: allows for externally calculated SHAP values, i.e., FastSHAP...</p>"},{"location":"api/interpretability/#drn.kernel_shap_explainer.KernelSHAP_DRN.shap_glm_values","title":"<code>shap_glm_values()</code>","text":"<p>The raw Kernel SHAP (GLM) output.</p>"},{"location":"api/interpretability/#drn.kernel_shap_explainer.KernelSHAP_DRN.shap_values_mean_adjustments","title":"<code>shap_values_mean_adjustments()</code>","text":"<p>The SHAP values and feature names</p>"},{"location":"api/interpretability/#key-methods","title":"Key Methods","text":""},{"location":"api/interpretability/#plot-adjustment-factors","title":"Plot Adjustment Factors","text":""},{"location":"api/interpretability/#drn.interpretability.DRNExplainer.plot_adjustment_factors","title":"<code>drn.interpretability.DRNExplainer.plot_adjustment_factors(instance, observation=None, cutpoints=None, num_interpolations=None, other_df_models=None, model_names=None, percentiles=None, cutpoints_label_bool=False, synthetic_data=None, plot_adjustments_labels=True, axes=None, x_range=None, y_range=None, plot_title=None, plot_mean_adjustment=False, plot_y_label=None, density_transparency=1.0, figsize=None)</code>","text":"<p>Plot the adjustment factors for each of the partitioned interval. expand: interpolation of cutpoints for density evaluations.</p>"},{"location":"api/interpretability/#plot-distributional-property-adjustment-with-shap","title":"Plot Distributional Property Adjustment with SHAP","text":""},{"location":"api/interpretability/#drn.interpretability.DRNExplainer.plot_dp_adjustment_shap","title":"<code>drn.interpretability.DRNExplainer.plot_dp_adjustment_shap(instance_raw, dist_property='Mean', quantile_bounds=None, method='Kernel', nsamples_background_fraction=1.0, top_K_features=3, adjustment=True, other_df_models=None, model_names=None, cutpoints=None, num_interpolations=None, labelling_gap=0.05, synthetic_data=None, synthetic_data_samples=int(1000000.0), observation=None, plot_baseline=True, x_range=None, y_range=None, plot_y_label=None, plot_title=None, figsize=None, density_transparency=1.0, shap_fontsize=25, legend_loc='upper left')</code>","text":"<p>Plot SHAP value-based adjustments with an option to include density functions.</p> <p>Args:     instance_raw: Raw data before one-hot encoding, used for instance-specific analysis.     dist_property: Distributional property to adjust ('Mean', 'Variance', 'Quantile').     method: Technique for SHAP value computation ('Kernel', 'Tree', 'FastSHAP', etc.).     nsamples_background_fraction: Fraction of background data for SHAP calculation.     top_K_features: Number of top features based on SHAP values.     adjustment: Whether to plot the SHAP values of the adjusted or unadjusted distributional property     other_df_models: Other distributional forecasting models for comparison.     model_names: Names of the other distributional forecasting models.     cutpoints: Cutpoints for partitioning feature space, defaults to <code>self.default_cutpoints</code>.     num_interpolations: Number of points for density interpolation, defaults to 2000.     labelling_gap: Gap between labels in the plot for readability.     synthetic_data, synthetic_data_samples: Synthetic data function for true density comparison and number of samples generated.     observation: Specific observation value for vertical line plotting.     plot_baseline: Flag to include baseline model's density plot.     x_range, y_range: Axis ranges for the plot.     plot_y_label, plot_title: Custom labels for the plot's axes and title.     density_transparency: Alpha value for density plot transparency.     shap_fontsize, figsize, label_adjustment_factor: Plot styling parameters.     legend_loc: Location of the legend in the plot.</p>"},{"location":"api/interpretability/#cdf-plot","title":"CDF Plot","text":""},{"location":"api/interpretability/#drn.interpretability.DRNExplainer.cdf_plot","title":"<code>drn.interpretability.DRNExplainer.cdf_plot(instance, grid=None, cutpoints=None, other_df_models=None, model_names=None, synthetic_data=None, x_range=None, plot_title=None, plot_baseline=True, density_transparency=1.0, dist_property='Mean', quantile_bounds=None, nsamples_background_fraction=0.1, adjustment=True, method='Kernel', labelling_gap=0.01, top_K_features=3, y_range=None, shap_fontsize=25, figsize=None)</code>","text":"<p>Plot the cumulative distribution function.</p>"},{"location":"api/metrics/","title":"Metrics","text":"<p>Evaluation metrics for distributional regression models, including both traditional point prediction metrics and distribution-aware measures.</p>"},{"location":"api/metrics/#distributional-metrics","title":"Distributional Metrics","text":""},{"location":"api/metrics/#crps-continuous-ranked-probability-score","title":"CRPS (Continuous Ranked Probability Score)","text":""},{"location":"api/metrics/#drn.metrics.crps","title":"<code>drn.metrics.crps(obs, grid, cdf_on_grid)</code>","text":"<p>Compute CRPS using the provided grid and CDF values with PyTorch tensors.</p> <p>:param obs: observed value(s) :param grid: a grid over y values :param cdf_on_grid: tensor of corresponding CDF values or a 2D tensor where each column is a CDF :return: CRPS value(s) as a PyTorch tensor</p>"},{"location":"api/metrics/#quantile-score","title":"Quantile Score","text":""},{"location":"api/metrics/#drn.metrics.quantile_score","title":"<code>drn.metrics.quantile_score(y_true, y_pred, p)</code>","text":"<p>Compute the quantile score for predictions at a specific quantile.</p> <p>:param y_true: Actual target values as a Pandas Series or PyTorch tensor. :param y_pred: Predicted target values as a numpy array or PyTorch tensor. :param p: The cumulative probability as a float :return: The quantile score as a PyTorch tensor.</p>"},{"location":"api/metrics/#quantile-losses","title":"Quantile Losses","text":""},{"location":"api/metrics/#drn.metrics.quantile_losses","title":"<code>drn.metrics.quantile_losses(p, model, model_name, X, y, max_iter=1000, tolerance=5e-05, l=None, u=None, print_score=True)</code>","text":"<p>Calculate and optionally print the quantile loss for the given data and model.</p> <p>:param p: The cumulative probability ntile as a float :param model: The trained model. :param model_name: The name of the trained model. :param X: Input features as a Pandas DataFrame or numpy array. :param y: True target values as a Pandas Series or numpy array. :param max_iter: The maximum number of iterations for the quantile search algorithm. :param tolerance: The tolerance for convergence of the the quantile search algorithm. :param l: The lower bound for the quantile search :param u: The upper bound for the quantile search :param print_score: A boolean indicating whether to print the score. :return: The quantile loss as a PyTorch tensor.</p>"},{"location":"api/metrics/#point-prediction-metrics","title":"Point Prediction Metrics","text":""},{"location":"api/metrics/#rmse-root-mean-squared-error","title":"RMSE (Root Mean Squared Error)","text":""},{"location":"api/metrics/#drn.metrics.rmse","title":"<code>drn.metrics.rmse(y, y_hat)</code>","text":"<p>Compute the Root Mean Square Error (RMSE) between the true values and predictions.</p> <p>:param y: True target values. Can be a Pandas Series or a PyTorch tensor. :param y_hat: Predicted target values. Should be a PyTorch tensor. :return: The RMSE as a PyTorch tensor.</p>"},{"location":"api/metrics/#utility-functions","title":"Utility Functions","text":""},{"location":"api/metrics/#binary-search-for-quantiles","title":"Binary Search for Quantiles","text":""},{"location":"api/metrics/#drn.utils.binary_search_icdf","title":"<code>drn.utils.binary_search_icdf(distribution, p, l=None, u=None, max_iter=1000, tolerance=1e-07)</code>","text":"<p>Generic binary search implementation for inverse CDF (quantiles).</p> <p>This function can be used by any distribution that has a <code>cdf</code> method but doesn't have its own <code>icdf</code> implementation.</p> <p>Args:     distribution: Distribution object with a <code>cdf</code> method     p: cumulative probability value at which to evaluate icdf     l: lower bound for the quantile search     u: upper bound for the quantile search     max_iter: maximum number of iterations     tolerance: stopping criteria for convergence</p> <p>Returns:     A tensor of shape (1, batch_shape) containing the inverse CDF values.</p>"},{"location":"api/training/","title":"Training","text":"<p>Training framework and utilities for DRN models, including the main training loop, loss functions, and optimization helpers.</p>"},{"location":"api/training/#main-training-function","title":"Main Training Function","text":""},{"location":"api/training/#drn.train.train","title":"<code>drn.train.train(model, train_dataset, val_dataset, epochs=200, patience=5, lr=None, device=None, log_interval=10, batch_size=128, optimizer=torch.optim.Adam, print_details=True, keep_best=True, gradient_clipping=False)</code>","text":"<p>A generic neural network training function given a model and datasets. Args:     model: The model to train.     train_dataset: Dataset for training.     val_dataset: Dataset for validation.     epochs: Number of epochs to train for.     patience: Number of epochs with no improvement after which training will be stopped.     lr: Learning rate for the optimizer.     device: Device to use for training (default is determined automatically).     log_interval: How often to log training progress.     batch_size: Batch size for training and validation.     optimizer: Optimizer class to use (default is Adam).     print_details: Whether to print detailed logs during training.     keep_best: Whether to return the best model found during training.     gradient_clipping: Whether to apply gradient clipping.</p>"},{"location":"api/training/#loss-functions","title":"Loss Functions","text":""},{"location":"api/training/#drn-loss","title":"DRN Loss","text":""},{"location":"api/training/#drn.models.drn_loss","title":"<code>drn.models.drn_loss(pred, y, kind='jbce', kl_alpha=0.0, mean_alpha=0.0, tv_alpha=0.0, dv_alpha=0.0, kl_direction='forwards')</code>","text":""},{"location":"api/training/#jbce-loss","title":"JBCE Loss","text":""},{"location":"api/training/#drn.models.ddr.jbce_loss","title":"<code>drn.models.ddr.jbce_loss(dists, y, alpha=0.0)</code>","text":"<p>The joint binary cross entropy loss. Args:     dists: the predicted distributions     y: the observed values     alpha: the penalty parameter</p>"},{"location":"api/training/#nll-loss","title":"NLL Loss","text":""},{"location":"api/training/#drn.models.ddr.nll_loss","title":"<code>drn.models.ddr.nll_loss(dists, y, alpha=0.0)</code>","text":""},{"location":"api/training/#model-utilities","title":"Model Utilities","text":""},{"location":"api/training/#cutpoint-generation","title":"Cutpoint Generation","text":""},{"location":"api/training/#drn.models.drn_cutpoints","title":"<code>drn.models.drn_cutpoints(c_0, c_K, y, proportion=None, num_cutpoints=None, min_obs=1)</code>","text":""},{"location":"api/training/#glm-utilities","title":"GLM Utilities","text":""},{"location":"api/training/#drn.models.glm.gaussian_deviance_loss","title":"<code>drn.models.glm.gaussian_deviance_loss(y_pred, y_true)</code>","text":"<p>Calculate the Normal deviance loss for the Gaussian distribution. Args:     y_pred: the predicted values (shape: (n,))     y_true: the observed values (shape: (n,)) Returns:     the deviance loss (shape: (,))</p>"},{"location":"api/training/#drn.models.glm.gamma_deviance_loss","title":"<code>drn.models.glm.gamma_deviance_loss(y_pred, y_true)</code>","text":"<p>Calculate the Tweedie deviance loss for the gamma distribution. Args:     y_pred: the predicted values (shape: (n,))     y_true: the observed values (shape: (n,)) Returns:     the deviance loss (shape: (,))</p>"},{"location":"api/utils/","title":"Utilities","text":"<p>Data preprocessing, splitting, and utility functions for DRN workflows.</p>"},{"location":"api/utils/#data-preprocessing","title":"Data Preprocessing","text":""},{"location":"api/utils/#split-and-preprocess","title":"Split and Preprocess","text":"<p>Main function for data splitting and preprocessing with support for both numerical and categorical features.</p>"},{"location":"api/utils/#drn.utils.split_and_preprocess","title":"<code>drn.utils.split_and_preprocess(features, target, num_features, cat_features, seed=42, num_standard=True)</code>","text":""},{"location":"api/utils/#data-splitting","title":"Data Splitting","text":""},{"location":"api/utils/#drn.utils.split_data","title":"<code>drn.utils.split_data(features, target, seed=42, train_size=0.6, val_size=0.2)</code>","text":"<p>Split features and target into train, validation, and test sets based on fractions of the entire dataset.</p> <p>Args:     features: DataFrame of predictors.     target: Series of labels.     seed: Random seed for reproducibility.     train_size: Fraction of data for training.     val_size: Fraction of data for validation.         (test_size is computed as 1 - train_size - val_size) Returns:     x_train_raw, x_val_raw, x_test_raw,     y_train, y_val, y_test</p>"},{"location":"api/utils/#data-preprocessing_1","title":"Data Preprocessing","text":""},{"location":"api/utils/#drn.utils.preprocess_data","title":"<code>drn.utils.preprocess_data(x_train_raw, x_val_raw, x_test_raw, num_features, cat_features, num_standard=True)</code>","text":"<p>Fit a ColumnTransformer on x_train_raw and transform raw splits. - Numeric features are optionally standardized. - Categorical features are one-hot encoded, using full categories detected from splits.</p> <p>Returns:     x_train, x_val, x_test, fitted ColumnTransformer, all_categories mapping</p>"},{"location":"api/utils/#categorical-handling","title":"Categorical Handling","text":""},{"location":"api/utils/#replace-rare-categories","title":"Replace Rare Categories","text":""},{"location":"api/utils/#drn.utils.replace_rare_categories","title":"<code>drn.utils.replace_rare_categories(df, threshold=10, placeholder='OTHER', cat_features=None)</code>","text":"<p>Replace rare categories in specified categorical columns with a placeholder category.</p> <p>Parameters: - df: The input DataFrame. - threshold: Minimum number of occurrences for a category to be kept. - placeholder: Name to assign to rare categories. - cat_features: If specified, only apply to these columns.</p> <p>Raises: - ValueError: If the placeholder value already exists in any of the target columns.</p> <p>Returns: - pd.DataFrame: A new DataFrame with rare categories replaced.</p>"},{"location":"api/utils/#mathematical-utilities","title":"Mathematical Utilities","text":""},{"location":"api/utils/#binary-search-for-inverse-cdf","title":"Binary Search for Inverse CDF","text":""},{"location":"api/utils/#drn.utils.binary_search_icdf","title":"<code>drn.utils.binary_search_icdf(distribution, p, l=None, u=None, max_iter=1000, tolerance=1e-07)</code>","text":"<p>Generic binary search implementation for inverse CDF (quantiles).</p> <p>This function can be used by any distribution that has a <code>cdf</code> method but doesn't have its own <code>icdf</code> implementation.</p> <p>Args:     distribution: Distribution object with a <code>cdf</code> method     p: cumulative probability value at which to evaluate icdf     l: lower bound for the quantile search     u: upper bound for the quantile search     max_iter: maximum number of iterations     tolerance: stopping criteria for convergence</p> <p>Returns:     A tensor of shape (1, batch_shape) containing the inverse CDF values.</p>"},{"location":"api/utils/#helper-functions","title":"Helper Functions","text":""},{"location":"api/utils/#convert-to-numpy","title":"Convert to NumPy","text":""},{"location":"api/utils/#drn.utils._to_numpy","title":"<code>drn.utils._to_numpy(data)</code>","text":"<p>Convert input data to numpy array with float32 precision.</p>"},{"location":"api/models/","title":"Models Overview","text":"<p>Core distributional regression models in the DRN package. All models inherit from the common <code>BaseModel</code> interface and provide distributional predictions.</p>"},{"location":"api/models/#model-hierarchy","title":"Model Hierarchy","text":"<pre><code>classDiagram\n    BaseModel &lt;|-- GLM\n    BaseModel &lt;|-- DeepGLM\n    BaseModel &lt;|-- DRN\n    BaseModel &lt;|-- CANN\n    BaseModel &lt;|-- MDN\n    BaseModel &lt;|-- DDR\n    BaseModel &lt;|-- Constant\n\n    class BaseModel {\n        +fit(X, y)\n        +predict(X)\n        +quantiles(X, percentiles)\n        +loss(x, y)\n    }\n\n    class GLM {\n        +distribution: str\n        +clone()\n    }\n\n    class DRN {\n        +baseline: BaseModel\n        +cutpoints: list\n        +log_adjustments(x)\n    }</code></pre>"},{"location":"api/models/#quick-reference","title":"Quick Reference","text":"Model Purpose Key Features Best For BaseModel Abstract base class Common interface, PyTorch Lightning All models inherit from this GLM Generalized Linear Models Interpretable, statistical foundation Baseline models, simple relationships DeepGLM Deep Generalized Linear Model Neural feature learning + GLM head Nonlinear relationships, distributional outputs DRN Distributional Refinement Network Neural + interpretable baseline Complex distributions with interpretability CANN Combined Actuarial Neural Network Actuarial focus, separate parameter networks Insurance and actuarial applications MDN Mixture Density Network Multi-modal distributions Complex, multi-peaked data DDR Deep Distribution Regression Pure neural approach Maximum flexibility, no baseline constraint Constant Constant prediction Simple baseline Benchmarking, ablation studies"},{"location":"api/models/#common-usage-patterns","title":"Common Usage Patterns","text":""},{"location":"api/models/#basic-model-training","title":"Basic Model Training","text":"<pre><code>from drn import GLM, DRN\n\n# Train baseline\nbaseline = GLM('gamma')\nbaseline.fit(X_train, y_train)\n\n# Train refined model\ndrn_model = DRN(baseline)\ndrn_model.fit(X_train, y_train)\n</code></pre>"},{"location":"api/models/#distribution-families","title":"Distribution Families","text":""},{"location":"api/models/#glm-distributions","title":"GLM Distributions","text":"<ul> <li><code>gaussian</code> - Normal distribution for unbounded continuous data</li> <li><code>gamma</code> - Gamma distribution for positive continuous data</li> <li><code>inversegaussian</code> - Inverse Gaussian for positive data with right skew</li> <li><code>lognormal</code> - Log-normal for multiplicative processes</li> </ul>"},{"location":"api/models/#advanced-distributions","title":"Advanced Distributions","text":"<ul> <li>Histogram - Flexible discrete distributions</li> <li>Extended Histogram - Continuous extensions of histograms</li> <li>Mixture Models - Multi-component distributions</li> </ul>"},{"location":"api/models/#model-selection-guide","title":"Model Selection Guide","text":""},{"location":"api/models/#start-with-glm-when","title":"Start with GLM when:","text":"<ul> <li>You need interpretability</li> <li>Data follows standard distributions</li> <li>Baseline performance is adequate</li> <li>Statistical inference is required</li> </ul>"},{"location":"api/models/#use-drn-when","title":"Use DRN when:","text":"<ul> <li>GLM baseline is reasonable but not sufficient</li> <li>You need both flexibility and interpretability</li> <li>Complex distributional shapes are present</li> <li>Regularization control is important</li> </ul>"},{"location":"api/models/#consider-advanced-models-when","title":"Consider Advanced Models when:","text":"<ul> <li>CANN: Actuarial applications with domain knowledge</li> <li>MDN: Multi-modal or mixture distributions expected</li> <li>DDR: Maximum flexibility, no interpretability needed</li> <li>Constant: Simple benchmarking baseline</li> </ul>"},{"location":"api/models/#model-comparison","title":"Model Comparison","text":""},{"location":"api/models/#performance-characteristics","title":"Performance Characteristics","text":"Model Training Speed Inference Speed Memory Usage Interpretability Flexibility GLM \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 DeepGLM \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 DRN \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 CANN \u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 MDN \u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 DDR \u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50"},{"location":"api/models/base/","title":"BaseModel","text":"<p>The abstract foundation for all distributional regression models in DRN. Provides unified interface, PyTorch Lightning integration, and common functionality.</p>"},{"location":"api/models/base/#class-definition","title":"Class Definition","text":"<p>               Bases: <code>LightningModule</code>, <code>ABC</code></p>"},{"location":"api/models/base/#drn.models.base.BaseModel-functions","title":"Functions","text":""},{"location":"api/models/base/#drn.models.base.BaseModel.icdf","title":"<code>icdf(x, p, l=None, u=None, max_iter=1000, tolerance=1e-07)</code>","text":"<p>Calculate the inverse CDF (quantiles) of the distribution for the given cumulative probability.</p> <p>This is a fallback implementation for PyTorch distributions that don't have icdf implemented.</p> <p>Args:     x: Input features     p: cumulative probability value at which to evaluate icdf     l: lower bound for the quantile search     u: upper bound for the quantile search     max_iter: maximum number of iterations     tolerance: stopping criteria</p> <p>Returns:     A tensor of shape (1, batch_shape) containing the inverse CDF values.</p>"},{"location":"api/models/base/#drn.models.base.BaseModel.predict","title":"<code>predict(x_raw)</code>","text":"<p>Creates a distributional prediction for the input data. Here, <code>x_raw</code> is raw data, and this method will apply any model-specific preprocessing.</p>"},{"location":"api/models/base/#drn.models.base.BaseModel.preprocess","title":"<code>preprocess(x, targets=False)</code>","text":"<p>Convert input data to a PyTorch tensor. Apply any neural network preprocessing (if applicable).</p>"},{"location":"api/models/base/#drn.models.base.BaseModel.quantiles","title":"<code>quantiles(x, percentiles, l=None, u=None, max_iter=1000, tolerance=1e-07)</code>","text":"<p>Calculate the quantile values for the given observations and percentiles (cumulative probabilities * 100).</p> <p>This unified implementation first checks if the distribution has its own quantiles method, then falls back to icdf-based approach.</p>"},{"location":"api/models/base/#overview","title":"Overview","text":"<p><code>BaseModel</code> is the abstract base class that all DRN models inherit from. It provides:</p> <ul> <li>Unified Interface - Consistent <code>.fit()</code>, <code>.predict()</code>, and <code>.quantiles()</code> methods</li> <li>PyTorch Lightning Integration - Built-in training loops, early stopping, checkpointing</li> <li>Automatic Data Handling - Conversion between pandas/numpy and PyTorch tensors</li> <li>GPU Support - Automatic device detection and tensor placement</li> <li>Distribution Interface - Common methods for working with distributional predictions</li> </ul>"},{"location":"api/models/base/#key-methods","title":"Key Methods","text":""},{"location":"api/models/base/#training-methods","title":"Training Methods","text":""},{"location":"api/models/base/#fitx_train-y_train-x_valnone-y_valnone-kwargs","title":"<code>fit(X_train, y_train, X_val=None, y_val=None, **kwargs)</code>","text":"<p>Main training method that handles the complete training workflow:</p> <ul> <li>Data preprocessing and tensor conversion</li> <li>DataLoader creation with batching and shuffling  </li> <li>Validation setup with early stopping (if validation data provided)</li> <li>Model checkpointing and best weight restoration</li> <li>GPU acceleration when available</li> </ul> <p>Parameters: - <code>X_train, y_train</code> - Training features and targets (pandas/numpy/tensor) - <code>X_val, y_val</code> - Optional validation data for early stopping - <code>batch_size</code> - Training batch size (default: 128) - <code>epochs</code> - Maximum training epochs (default: 10) - <code>patience</code> - Early stopping patience (default: 5) - <code>**trainer_kwargs</code> - Additional PyTorch Lightning trainer arguments</p>"},{"location":"api/models/base/#prediction-methods","title":"Prediction Methods","text":""},{"location":"api/models/base/#predictx_raw","title":"<code>predict(x_raw)</code>","text":"<p>Creates distributional predictions from input features:</p> <ul> <li>Automatic preprocessing - Handles pandas/numpy to tensor conversion</li> <li>Returns distribution objects - With <code>.mean</code>, <code>.quantiles()</code>, <code>.cdf()</code>, etc.</li> <li>Consistent interface - Same API across all model types</li> </ul>"},{"location":"api/models/base/#quantilesx-percentiles-lnone-unone-kwargs","title":"<code>quantiles(x, percentiles, l=None, u=None, **kwargs)</code>","text":"<p>Unified quantile calculation supporting all distribution types:</p> <ul> <li>Multiple percentiles - Calculate several quantiles at once</li> <li>Bounds specification - Optional lower/upper bounds for search</li> <li>Automatic fallback - Uses binary search when analytic quantiles unavailable</li> </ul>"},{"location":"api/models/base/#abstract-methods","title":"Abstract Methods","text":"<p>Subclasses must implement:</p>"},{"location":"api/models/base/#lossx-y-torchtensor","title":"<code>loss(x, y) -&gt; torch.Tensor</code>","text":"<p>Define the loss function for model training.</p>"},{"location":"api/models/base/#_predictx-distribution","title":"<code>_predict(x) -&gt; Distribution</code>","text":"<p>Core prediction logic returning PyTorch distribution objects.</p>"},{"location":"api/models/cann/","title":"CANN - Combined Actuarial Neural Network","text":"<p>Advanced neural network architecture designed specifically for actuarial and insurance applications.</p>"},{"location":"api/models/cann/#class-definition","title":"Class Definition","text":"<p>               Bases: <code>BaseModel</code></p> <p>The Combined Actuarial Neural Network (CANN) model adaptable for both gamma and Gaussian GLMs.</p>"},{"location":"api/models/cann/#drn.models.cann.CANN-functions","title":"Functions","text":""},{"location":"api/models/cann/#drn.models.cann.CANN.__init__","title":"<code>__init__(baseline, num_hidden_layers=2, hidden_size=50, dropout_rate=0.2, train_glm=False, learning_rate=0.001)</code>","text":"<p>Args:     baseline_model: the baseline model to use (GLM or Constant)     num_hidden_layers: the number of hidden layers in the neural network     hidden_size: the number of neurons in each hidden layer     train_glm: whether to retrain the baseline model or not</p>"},{"location":"api/models/cann/#drn.models.cann.CANN.forward","title":"<code>forward(x)</code>","text":"<p>Calculate the predicted outputs for the distributions. Args:     x: the input features (shape: (n, p)) Returns:     the predicted outputs (shape: (n,))</p>"},{"location":"api/models/cann/#drn.models.cann.CANN.mean","title":"<code>mean(x)</code>","text":"<p>Calculate the predicted means for the given observations, specific to the model type.</p>"},{"location":"api/models/constant/","title":"Constant - Constant Prediction Model","text":"<p>Simple baseline model that predicts constant distributions for benchmarking and ablation studies.</p>"},{"location":"api/models/constant/#class-definition","title":"Class Definition","text":"<p>               Bases: <code>BaseModel</code></p> <p>A baseline model that predicts a constant distribution, ignoring covariates.</p> <ul> <li>Before fit(): mean = 1 (Gamma/IG), 0 (Gaussian); dispersion = 1.</li> <li>fit(): ignores X, sets mean to y_train.mean(), estimates dispersion via estimate_dispersion.</li> </ul>"},{"location":"api/models/constant/#drn.models.constant.Constant-functions","title":"Functions","text":""},{"location":"api/models/constant/#drn.models.constant.Constant.clone","title":"<code>clone()</code>","text":"<p>Create a copy of the model with the same parameters.</p>"},{"location":"api/models/constant/#drn.models.constant.Constant.predict","title":"<code>predict(x)</code>","text":"<p>Create a (constant) distributional forecast for the given inputs.</p>"},{"location":"api/models/constant/#overview","title":"Overview","text":"<p>The Constant model provides: - Simple baselines - Predict same distribution for all inputs - Benchmarking - Performance comparison baseline - Ablation studies - Control model for feature importance - Sanity checks - Verify that other models add value</p>"},{"location":"api/models/constant/#key-features","title":"Key Features","text":"<ul> <li>Distribution fitting - Fits single distribution to all training data</li> <li>Fast inference - No feature processing required</li> <li>Multiple distributions - Supports Gaussian, Gamma, etc.</li> <li>Debugging tool - Helps identify modeling issues</li> </ul>"},{"location":"api/models/constant/#quick-example","title":"Quick Example","text":"<pre><code>from drn.models import Constant\n\n# Fit constant Gamma distribution\nconstant_model = Constant(distribution='gamma')\nconstant_model.fit(X_train, y_train)\n\n# All predictions are identical\npred1 = constant_model.predict(X_test[:1])\npred2 = constant_model.predict(X_test[100:101])\n\n# Same distribution parameters\nassert torch.allclose(pred1.mean, pred2.mean)\nassert torch.allclose(pred1.variance, pred2.variance)\n</code></pre>"},{"location":"api/models/constant/#use-cases","title":"Use Cases","text":""},{"location":"api/models/constant/#benchmarking","title":"Benchmarking","text":"<pre><code>from drn.metrics import rmse, crps\n\n# Compare against constant baseline\nconstant_baseline = Constant('gamma')\nconstant_baseline.fit(X_train, y_train)\n\nyour_model = SomeModel()\nyour_model.fit(X_train, y_train)\n\n# Evaluate both\nconstant_pred = constant_baseline.predict(X_test)\nyour_pred = your_model.predict(X_test)\n\nconstant_rmse = rmse(y_test, constant_pred.mean)\nyour_rmse = rmse(y_test, your_pred.mean)\n\nprint(f\"Constant baseline RMSE: {constant_rmse:.4f}\")\nprint(f\"Your model RMSE: {your_rmse:.4f}\")\nprint(f\"Improvement: {((constant_rmse - your_rmse) / constant_rmse * 100):.1f}%\")\n</code></pre>"},{"location":"api/models/constant/#performance-characteristics","title":"Performance Characteristics","text":"<p>The Constant model excels in simplicity and speed:</p> Aspect Rating Notes Training Speed \u2b50\u2b50\u2b50\u2b50\u2b50 Instant - just fits distribution to data Memory Usage \u2b50\u2b50\u2b50\u2b50\u2b50 Minimal - stores only distribution parameters Flexibility \u2b50 None - same prediction for all inputs Interpretability \u2b50\u2b50\u2b50\u2b50\u2b50 Perfect - single fitted distribution Stability \u2b50\u2b50\u2b50\u2b50\u2b50 Maximum - no complexity to cause instability"},{"location":"api/models/constant/#supported-distributions","title":"Supported Distributions","text":"<p>The Constant model supports various distributions, but can be easily extended:</p>"},{"location":"api/models/constant/#continuous-distributions","title":"Continuous Distributions","text":"<pre><code># Gaussian for unbounded data\nconstant_gaussian = Constant('gaussian')\n\n# Gamma for positive data  \nconstant_gamma = Constant('gamma')\n\n# Log-normal for multiplicative processes\nconstant_lognormal = Constant('lognormal')\n\n# Inverse Gaussian for right-skewed positive data\nconstant_ig = Constant('inversegaussian')\n</code></pre>"},{"location":"api/models/ddr/","title":"DDR - Deep Distribution Regression","text":"<p>The deep distributional regression model.</p>"},{"location":"api/models/ddr/#class-definition","title":"Class Definition","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/models/ddr/#drn.models.ddr.DDR-functions","title":"Functions","text":""},{"location":"api/models/ddr/#drn.models.ddr.DDR.__init__","title":"<code>__init__(cutpoints=None, num_hidden_layers=2, hidden_size=100, dropout_rate=0.2, proportion=0.1, loss_metric='jbce', learning_rate=0.001)</code>","text":"<p>Args:     x_train_shape: The shape of the training data, used to define the input size of the first layer.     cutpoints: The cutpoints for the DDR model.     num_hidden_layers: The number of hidden layers in the network.     hidden_size: The number of neurons in each hidden layer.</p>"},{"location":"api/models/ddr/#drn.models.ddr.DDR.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the DDR model. Args:     x: Input tensor. Returns:     The cutpoints and probabilities for the DDR model.</p>"},{"location":"api/models/ddr/#loss-functions","title":"Loss Functions","text":""},{"location":"api/models/ddr/#jbce-loss","title":"JBCE Loss","text":"<p>The joint binary cross entropy loss. Args:     dists: the predicted distributions     y: the observed values     alpha: the penalty parameter</p>"},{"location":"api/models/ddr/#nll-loss","title":"NLL Loss","text":""},{"location":"api/models/deepglm/","title":"DeepGLM - Deep Generalized Linear Model","text":"<p>A neural network approach to distributional regression that combines the interpretable structure of GLMs with the flexibility of deep learning. DeepGLM learns nonlinear feature representations while maintaining distributional outputs.</p>"},{"location":"api/models/deepglm/#class-definition","title":"Class Definition","text":"<p>               Bases: <code>BaseModel</code></p> <p>Deep Generalized Linear Model (DeepGLM).</p> <p>The model learns a nonlinear representation of the inputs via a feed-forward neural network and then applies a GLM head to produce the conditional mean. A fixed dispersion parameter is estimated after training via a classical deviance-based estimator (see <code>estimate_dispersion</code>).</p> <p>Supported response distributions:     - 'gamma' (log link)     - 'gaussian' (identity link)     - 'inversegaussian' (log link)     - 'lognormal' (identity on log-scale parameter; distributional head is LogNormal)</p>"},{"location":"api/models/deepglm/#overview","title":"Overview","text":"<p>The <code>DeepGLM</code> class extends traditional GLMs by learning nonlinear feature transformations through a feed-forward neural network before applying the GLM head. Key features:</p> <ul> <li>Nonlinear Feature Learning - Multi-layer neural network for complex patterns</li> <li>Distributional Outputs - Full probability distributions, not just point predictions</li> <li>Multiple Distribution Families - Gamma, Gaussian, and Inverse Gaussian support</li> <li>End-to-End Training - Unified loss function combining representation learning and GLM fitting</li> <li>Flexible Architecture - Configurable network depth and width</li> <li>Post-hoc Dispersion Estimation - Classical statistical estimation after neural training</li> </ul>"},{"location":"api/models/deepglm/#supported-distributions","title":"Supported Distributions","text":""},{"location":"api/models/deepglm/#gamma-distribution","title":"Gamma Distribution","text":"<p><pre><code>deepglm = DeepGLM('gamma', num_hidden_layers=2, hidden_size=128)\n</code></pre> - Use Case: Positive continuous data with right skew - Link Function: Log (<code>log(\u03bc) = \u03b7</code>) - Best For: Insurance claims, sales amounts, service times - Output: Gamma distribution with learned mean and estimated dispersion</p>"},{"location":"api/models/deepglm/#gaussian-distribution","title":"Gaussian Distribution","text":"<p><pre><code>deepglm = DeepGLM('gaussian', num_hidden_layers=2, hidden_size=128)\n</code></pre> - Use Case: Continuous data with complex nonlinear patterns - Link Function: Identity (<code>\u03bc = \u03b7</code>) - Best For: Complex regression tasks with symmetric errors - Output: Normal distribution with learned mean and estimated variance</p>"},{"location":"api/models/deepglm/#inverse-gaussian-distribution","title":"Inverse Gaussian Distribution","text":"<p><pre><code>deepglm = DeepGLM('inversegaussian', num_hidden_layers=2, hidden_size=128)\n</code></pre> - Use Case: Positive data with extreme right tail and nonlinear relationships - Link Function: Log (<code>log(\u03bc) = \u03b7</code>) - Best For: First passage times, duration modeling with complex covariates - Output: Inverse Gaussian distribution with learned parameters</p>"},{"location":"api/models/deepglm/#quick-start","title":"Quick Start","text":""},{"location":"api/models/deepglm/#basic-usage","title":"Basic Usage","text":"<pre><code>from drn import DeepGLM\nimport pandas as pd\nimport numpy as np\n\n# Load data with complex nonlinear relationships\nX = pd.DataFrame({\n    'age': np.random.uniform(20, 80, 1000),\n    'income': np.random.uniform(20000, 100000, 1000),\n    'risk_score': np.random.uniform(0, 1, 1000)\n})\n\n# Create complex nonlinear target\ny = np.exp(\n    0.1 * X['age'] + \n    0.5 * np.log(X['income']) + \n    2.0 * X['risk_score']**2 +\n    np.random.gamma(2, 0.5, 1000)\n)\n\n# Train DeepGLM\ndeepglm = DeepGLM(\n    distribution='gamma',\n    num_hidden_layers=3,\n    hidden_size=64,\n    dropout_rate=0.1,\n    learning_rate=1e-3\n)\n\ndeepglm.fit(X, y, epochs=100, batch_size=128)\n\n# Make distributional predictions\npredictions = deepglm.predict(X_test)\nmean_pred = predictions.mean\npercentiles = deepglm.quantiles(X_test, [10, 50, 90])\n</code></pre>"},{"location":"api/models/deepglm/#comparison-with-traditional-glm","title":"Comparison with Traditional GLM","text":"<pre><code>from drn import GLM, DeepGLM\nfrom drn.metrics import rmse, crps\n\n# Train traditional GLM\ntraditional_glm = GLM('gamma')\ntraditional_glm.fit(X_train, y_train)\n\n# Train DeepGLM\ndeep_glm = DeepGLM('gamma', num_hidden_layers=2, hidden_size=64)\ndeep_glm.fit(X_train, y_train, X_test, y_test, epochs=100)\n\n# Compare predictions\nglm_pred = traditional_glm.predict(X_test)\ndeep_pred = deep_glm.predict(X_test)\n\n# Evaluation metrics\nmetrics = {}\nfor name, pred in [('GLM', glm_pred), ('DeepGLM', deep_pred)]:\n    metrics[name] = {\n        'rmse': rmse(y_test, pred.mean).item(),\n        'nll': -pred.log_prob(torch.tensor(y_test.values)).mean().item()\n    }\n\nprint(\"Model Comparison:\")\nprint(f\"{'Model':&lt;10} {'RMSE':&lt;10} {'NLL':&lt;10}\")\nprint(\"-\" * 35)\nfor model, vals in metrics.items():\n    print(f\"{model:&lt;10} {vals['rmse']:&lt;10.3f} {vals['nll']:&lt;10.3f}\")\n</code></pre>"},{"location":"api/models/drn/","title":"DRN - Distributional Refinement Network","text":"<p>The main DRN model that combines interpretable baselines with flexible neural network refinements for advanced distributional forecasting.</p>"},{"location":"api/models/drn/#class-definition","title":"Class Definition","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/models/drn/#drn.models.drn.DRN-functions","title":"Functions","text":""},{"location":"api/models/drn/#drn.models.drn.DRN.__init__","title":"<code>__init__(baseline, cutpoints=None, ct=None, num_hidden_layers=2, hidden_size=75, dropout_rate=0.2, baseline_start=False, proportion=0.1, min_obs=1, loss_metric='jbce', kl_alpha=0.0, mean_alpha=0.0, tv_alpha=0.0, dv_alpha=0.0, kl_direction='forwards', learning_rate=0.001, debug=False)</code>","text":"<p>Args:     glm: A Generalized Linear Model (GLM) that DRN will adjust.     cutpoints: Cutpoints for the DRN model.     num_hidden_layers: Number of hidden layers in the DRN network.     hidden_size: Number of neurons in each hidden layer.</p>"},{"location":"api/models/drn/#drn.models.drn.DRN.log_adjustments","title":"<code>log_adjustments(x)</code>","text":"<p>Estimates log adjustments using the neural network. Args:     x: Input features. Returns:     Log adjustments for the DRN model.</p>"},{"location":"api/models/drn/#overview","title":"Overview","text":"<p>The Distributional Refinement Network (DRN) is the flagship model of this package. It addresses the fundamental challenge of building models that are both interpretable and flexible by:</p> <ol> <li>Starting with an interpretable baseline (typically a GLM)</li> <li>Adding neural refinements through a deep network</li> <li>Operating on discretized regions defined by cutpoints  </li> <li>Balancing flexibility with regularization through multiple penalty terms</li> </ol> <p>The result is a model that maintains the interpretability of the baseline while achieving superior distributional forecasting performance.</p>"},{"location":"api/models/drn/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TB\n    A[Input Features X] --&gt; B[Baseline Model]\n    A --&gt; C[Neural Network]\n\n    B --&gt; D[Baseline Distribution]\n    C --&gt; E[Log Adjustments]\n\n    D --&gt; F[Discretization via Cutpoints]\n    E --&gt; F\n\n    F --&gt; G[Refined Distribution]\n\n    G --&gt; H[Final Predictions]\n\n    I[KL Divergence] --&gt; F\n    J[Roughness Penalty] --&gt; F\n    K[Mean Penalty] --&gt; F</code></pre>"},{"location":"api/models/drn/#key-parameters","title":"Key Parameters","text":""},{"location":"api/models/drn/#model-architecture","title":"Model Architecture","text":"<p>The core model structure is controlled by the <code>baseline</code> model (typically a GLM), <code>cutpoints</code> for discretization, and neural network dimensions via <code>hidden_size</code> (default: 75) and <code>num_hidden_layers</code> (default: 2). Dropout regularization is applied at rate <code>dropout_rate</code> (default: 0.2).</p>"},{"location":"api/models/drn/#training-control","title":"Training Control","text":"<p>Set <code>baseline_start=True</code> to initialize neural weights at zero, effectively starting from the baseline model. The <code>learning_rate</code> defaults to 1e-3 for the Adam optimizer, and <code>loss_metric</code> can be either \"jbce\" or \"nll\".</p>"},{"location":"api/models/drn/#regularization-parameters","title":"Regularization Parameters","text":"<p>Three main penalty terms control model behavior: <code>kl_alpha</code> penalizes deviation from the baseline distribution, <code>mean_alpha</code> constrains mean predictions, and <code>dv_alpha</code> enforces density smoothness. The <code>kl_direction</code> parameter controls whether KL divergence is computed forwards or backwards.</p>"},{"location":"api/models/drn/#quick-start","title":"Quick Start","text":""},{"location":"api/models/drn/#basic-drn-training","title":"Basic DRN Training","text":"<pre><code>from drn import GLM, DRN, train\nfrom drn.models import drn_cutpoints, drn_loss\nimport torch\n\n# 1. Train interpretable baseline\nbaseline = GLM('gamma')\nbaseline.fit(X_train, y_train)\n\n# 2. Define refinement region  \ncutpoints = drn_cutpoints(\n    c_0=y_train.min() * 0.9,  # Lower bound should be below minimum\n    c_K=y_train.max() * 1.1,  # Upper bound should be above maximum  \n    proportion=0.1,           # 10% cutpoints-to-observation ratio\n    y=y_train,\n    min_obs=10               # Minimum observations per interval\n)\n\n# 3. Initialize DRN\ndrn_model = DRN(\n    baseline=baseline,\n    cutpoints=cutpoints,\n    hidden_size=128,\n    num_hidden_layers=2,\n    dropout_rate=0.1\n)\n\n# 4. Train with custom loss\ntrain_dataset = torch.utils.data.TensorDataset(\n    torch.tensor(X_train.values, dtype=torch.float32),\n    torch.tensor(y_train.values, dtype=torch.float32)\n)\nval_dataset = torch.utils.data.TensorDataset(\n    torch.tensor(X_val.values, dtype=torch.float32),\n    torch.tensor(y_val.values, dtype=torch.float32)\n)\n\ntrain(\n    drn_model,\n    lambda pred, y: drn_loss(\n        pred, y,\n        kl_alpha=1e-4,\n        dv_alpha=1e-3,\n        mean_alpha=1e-5\n    ),\n    train_dataset,\n    val_dataset,\n    epochs=100\n)\n</code></pre>"},{"location":"api/models/drn/#cutpoints-system","title":"Cutpoints System","text":"<p>The heart of DRN's flexibility lies in its cutpoints system, which discretizes the response space into refinement regions.</p>"},{"location":"api/models/drn/#cutpoint-generation","title":"Cutpoint Generation","text":""},{"location":"api/models/drn/#cutpoint-strategies","title":"Cutpoint Strategies","text":"<pre><code>from drn.models import drn_cutpoints\nimport numpy as np\n\n# Strategy 1: Quantile-based (recommended)\ncutpoints = drn_cutpoints(\n    c_0=y_train.quantile(0.01),  # 1st percentile lower bound\n    c_K=y_train.quantile(0.99),  # 99th percentile upper bound  \n    proportion=0.08,             # 8% cutpoints-to-observation ratio\n    y=y_train,\n    min_obs=15\n)\n\n# Strategy 2: Fixed bounds\ncutpoints = drn_cutpoints(\n    c_0=0,              # Fixed lower bound\n    c_K=1000,           # Fixed upper bound\n    proportion=0.1,\n    y=y_train,\n    min_obs=20\n)\n\n# Strategy 3: Data-driven bounds\nmargin = (y_train.max() - y_train.min()) * 0.1\ncutpoints = drn_cutpoints(\n    c_0=y_train.min() - margin,\n    c_K=y_train.max() + margin,\n    proportion=0.05,\n    y=y_train,\n    min_obs=25\n)\n\nprint(f\"Generated {len(cutpoints)} cutpoints\")\nprint(f\"Refinement range: [{cutpoints[0]:.2f}, {cutpoints[-1]:.2f}]\")\n</code></pre>"},{"location":"api/models/drn/#handling-different-data-ranges","title":"Handling Different Data Ranges","text":"<p>\u26a0\ufe0f Critical: The cutpoint bounds <code>c_0</code> and <code>c_K</code> must appropriately cover your data range:</p> <p>For Positive Data (insurance claims, prices, etc.): <pre><code># For data that can only be positive\ncutpoints = drn_cutpoints(\n    c_0=max(0, y_train.min() * 0.9),  # Ensure c_0 \u2265 0\n    c_K=y_train.max() * 1.1,\n    proportion=0.1,\n    y=y_train,\n    min_obs=10\n)\n</code></pre></p> <p>For Data Including Negative Values (profits/losses, temperature, etc.): <pre><code># For data that can be negative  \nmargin = (y_train.max() - y_train.min()) * 0.1\ncutpoints = drn_cutpoints(\n    c_0=y_train.min() - margin,  # Allow for values below observed minimum\n    c_K=y_train.max() + margin,  # Allow for values above observed maximum\n    proportion=0.1,\n    y=y_train,\n    min_obs=10\n)\n</code></pre></p> <p>Why this matters: DRN can only refine the predicted distributions within the cutpoint range <code>[c_0, c_K]</code>. If your data falls outside this range, predictions will revert to the baseline model.</p>"},{"location":"api/models/drn/#regularization","title":"Regularization","text":"<p>DRN uses three penalty terms to balance baseline adherence with neural flexibility.</p>"},{"location":"api/models/drn/#kl-divergence-control-kl_alpha","title":"KL Divergence Control (<code>kl_alpha</code>)","text":"<p>This parameter controls how much the refined distribution can deviate from the baseline. Values around 1e-5 maintain close adherence to the baseline, while 1e-4 provides moderate refinement, and 1e-3 allows aggressive deviations. The <code>kl_direction</code> parameter determines whether the penalty is computed as KL(baseline || drn) for 'forwards' or KL(drn || baseline) for 'backwards'.</p>"},{"location":"api/models/drn/#roughness-penalty-dv_alpha","title":"Roughness Penalty (<code>dv_alpha</code>)","text":"<p>Higher values (1e-2) enforce very smooth densities, while lower values (1e-4) permit complex density shapes. The default 1e-3 provides a reasonable balance between flexibility and smoothness.</p>"},{"location":"api/models/drn/#mean-penalty-mean_alpha","title":"Mean Penalty (<code>mean_alpha</code>)","text":"<p>This constrains how much the predicted mean can deviate from the baseline mean. Set to 1e-3 to force close adherence, 1e-5 for moderate constraint, or 0.0 to allow free mean adjustment.</p>"},{"location":"api/models/drn/#training-example","title":"Training Example","text":"<pre><code>from drn import GLM, DRN, train\nfrom drn.models import drn_cutpoints, drn_loss\nfrom drn.utils import split_and_preprocess\nimport torch\n\n# Load and preprocess data\nx_train, x_val, x_test, y_train, y_val, y_test, *_ = split_and_preprocess(\n    X, y, test_size=0.2, val_size=0.1, seed=42\n)\n\n# Train baseline and create DRN\nbaseline = GLM('gamma').fit(x_train, y_train)\ncutpoints = drn_cutpoints(\n    c_0=y_train.quantile(0.01), c_K=y_train.quantile(0.99),\n    proportion=0.08, y=y_train, min_obs=20\n)\n\ndrn_model = DRN(baseline, cutpoints, hidden_size=128, num_hidden_layers=2)\n\n# Train with regularized loss\ntrain_data = torch.utils.data.TensorDataset(\n    torch.tensor(x_train.values, dtype=torch.float32),\n    torch.tensor(y_train.values, dtype=torch.float32)\n)\nval_data = torch.utils.data.TensorDataset(\n    torch.tensor(x_val.values, dtype=torch.float32),\n    torch.tensor(y_val.values, dtype=torch.float32)\n)\n\nloss_fn = lambda pred, y: drn_loss(pred, y, kl_alpha=1e-4, dv_alpha=1e-3)\ntrain(drn_model, loss_fn, train_data, val_data, epochs=50)\n</code></pre>"},{"location":"api/models/drn/#advanced-features","title":"Advanced Features","text":""},{"location":"api/models/drn/#custom-loss-functions","title":"Custom Loss Functions","text":"<pre><code>def custom_drn_loss(pred_dist, y_true):\n    \"\"\"Custom loss with domain-specific penalties.\"\"\"\n\n    # Base DRN loss\n    base_loss = drn_loss(pred_dist, y_true, \n                        kl_alpha=1e-4, dv_alpha=1e-3, mean_alpha=1e-5)\n\n    # Custom penalty: discourage predictions below zero\n    mean_pred = pred_dist.mean  \n    negative_penalty = torch.mean(torch.relu(-mean_pred)) * 1e-2\n\n    # Custom penalty: encourage reasonable variance\n    if hasattr(pred_dist, 'variance'):\n        var_penalty = torch.mean(torch.relu(pred_dist.variance - y_true.var() * 5))\n        return base_loss + negative_penalty + var_penalty * 1e-4\n\n    return base_loss + negative_penalty\n</code></pre>"},{"location":"api/models/drn/#lazy-initialization","title":"Lazy Initialization","text":"<pre><code># DRN can automatically determine cutpoints during training\ndrn_model = DRN(\n    baseline=baseline,\n    cutpoints=None,         # Will be determined automatically\n    proportion=0.08,        # Cutpoints-to-observation ratio\n    min_obs=15,            # Minimum observations per interval\n    hidden_size=128\n)\n\n# Cutpoints are created during .fit() call\ndrn_model.fit(X_train, y_train)\nprint(f\"Auto-generated {len(drn_model.cutpoints)} cutpoints\")\n</code></pre>"},{"location":"api/models/drn/#multi-stage-training","title":"Multi-Stage Training","text":"<pre><code># Stage 1: High regularization for stable initialization\nstage1_loss = lambda pred, y: drn_loss(pred, y, \n                                      kl_alpha=1e-3, dv_alpha=1e-2, mean_alpha=1e-4)\ntrain(drn_model, stage1_loss, train_dataset, val_dataset, epochs=30)\n\n# Stage 2: Reduce regularization for flexibility  \nstage2_loss = lambda pred, y: drn_loss(pred, y,\n                                      kl_alpha=1e-4, dv_alpha=1e-3, mean_alpha=1e-5)\ntrain(drn_model, stage2_loss, train_dataset, val_dataset, epochs=50)\n\n# Stage 3: Fine-tuning with minimal regularization\nstage3_loss = lambda pred, y: drn_loss(pred, y,\n                                      kl_alpha=1e-5, dv_alpha=1e-4, mean_alpha=0)\ntrain(drn_model, stage3_loss, train_dataset, val_dataset, epochs=20)\n</code></pre>"},{"location":"api/models/drn/#hyperparameter-tuning-guide","title":"Hyperparameter Tuning Guide","text":""},{"location":"api/models/drn/#tuning-strategy","title":"Tuning Strategy","text":"<ol> <li>Start Conservative: <code>kl_alpha=1e-4, dv_alpha=1e-3, mean_alpha=1e-5</code></li> <li>Check Baseline Quality: If baseline is poor, decrease <code>kl_alpha</code></li> <li>Monitor Smoothness: If densities are jagged, increase <code>dv_alpha</code></li> <li>Adjust Complexity: More cutpoints = more flexibility but harder training</li> </ol>"},{"location":"api/models/drn/#systematic-grid-search","title":"Systematic Grid Search","text":"<pre><code>def tune_drn_hyperparameters(baseline, X_train, Y_train, X_val, Y_val):\n    \"\"\"Simple hyperparameter tuning for DRN.\"\"\"\n\n    best_score = float('inf')\n    best_params = None\n\n    # Define search grid\n    kl_alphas = [1e-5, 1e-4, 1e-3]\n    dv_alphas = [1e-4, 1e-3, 1e-2]  \n    hidden_sizes = [64, 128, 256]\n\n    for kl_alpha in kl_alphas:\n        for dv_alpha in dv_alphas:\n            for hidden_size in hidden_sizes:\n\n                # Create model\n                drn = DRN(baseline, cutpoints, hidden_size=hidden_size)\n\n                # Define loss\n                loss_fn = lambda pred, y: drn_loss(pred, y, \n                                                  kl_alpha=kl_alpha, \n                                                  dv_alpha=dv_alpha)\n\n                # Train\n                train(drn, loss_fn, train_dataset, val_dataset, epochs=30)\n\n                # Evaluate  \n                pred = drn.predict(X_val)\n                score = rmse(Y_val, pred.mean)\n\n                if score &lt; best_score:\n                    best_score = score\n                    best_params = {\n                        'kl_alpha': kl_alpha,\n                        'dv_alpha': dv_alpha, \n                        'hidden_size': hidden_size\n                    }\n\n    return best_params, best_score\n</code></pre>"},{"location":"api/models/drn/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/models/drn/#common-issues","title":"\u26a0\ufe0f Common Issues","text":""},{"location":"api/models/drn/#training-loss-not-decreasing","title":"Training Loss Not Decreasing","text":"<pre><code># Solutions:\n# 1. Lower learning rate\ndrn_model = DRN(baseline, cutpoints, learning_rate=1e-4)\n\n# 2. Reduce regularization  \nloss_fn = lambda pred, y: drn_loss(pred, y, kl_alpha=1e-5, dv_alpha=1e-4)\n\n# 3. Check baseline quality\nbaseline_pred = baseline.predict(X_val)\nbaseline_score = rmse(Y_val, baseline_pred.mean)\nprint(f\"Baseline validation RMSE: {baseline_score}\")\n</code></pre>"},{"location":"api/models/drn/#memory-issues","title":"Memory Issues","text":"<pre><code># Reduce batch size\ntrain(drn_model, loss_fn, train_dataset, val_dataset, batch_size=64)\n\n# Reduce model size\ndrn_model = DRN(baseline, cutpoints, hidden_size=64, num_hidden_layers=1)\n\n# Use gradient accumulation\n# Effective batch size = batch_size * accumulation_steps\n</code></pre>"},{"location":"api/models/drn/#overfitting","title":"Overfitting","text":"<pre><code># Increase regularization\nloss_fn = lambda pred, y: drn_loss(pred, y, kl_alpha=1e-3, dv_alpha=1e-2)\n\n# Increase dropout\ndrn_model = DRN(baseline, cutpoints, dropout_rate=0.3)\n\n# Reduce model capacity\ndrn_model = DRN(baseline, cutpoints, hidden_size=32, num_hidden_layers=1)\n</code></pre>"},{"location":"api/models/drn/#see-also","title":"See Also","text":"<ul> <li>BaseModel - Common model interface</li> <li>GLM - Baseline model implementation</li> <li>Training - Advanced training strategies</li> <li>Quick Start - Practical examples</li> <li>Advanced Usage - Custom training loops</li> </ul>"},{"location":"api/models/glm/","title":"GLM - Generalized Linear Models","text":"<p>Interpretable baseline models for distributional regression. Supports multiple distribution families with both statistical and neural network training approaches.</p>"},{"location":"api/models/glm/#class-definition","title":"Class Definition","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/models/glm/#drn.models.glm.GLM-functions","title":"Functions","text":""},{"location":"api/models/glm/#drn.models.glm.GLM.clone","title":"<code>clone()</code>","text":"<p>Create an independent copy of the model.</p>"},{"location":"api/models/glm/#drn.models.glm.GLM.mean","title":"<code>mean(x)</code>","text":"<p>Calculate the predicted means for the given observations.</p>"},{"location":"api/models/glm/#overview","title":"Overview","text":"<p>The <code>GLM</code> class provides PyTorch implementations of Generalized Linear Models with distributional outputs. Key features:</p> <ul> <li>Multiple Distribution Families - Gaussian, Gamma, Inverse Gaussian, Log-Normal</li> <li>Dual Training Modes - Statistical (statsmodels) or neural (PyTorch gradient descent)</li> <li>Interpretable Parameters - Access to coefficients, intercepts, and dispersion</li> <li>Seamless Integration - Perfect baseline for DRN refinement</li> <li>Automatic Parameter Estimation - Maximum likelihood via statsmodels</li> </ul>"},{"location":"api/models/glm/#supported-distributions","title":"Supported Distributions","text":""},{"location":"api/models/glm/#gaussian-distribution","title":"Gaussian Distribution","text":"<p><pre><code>glm = GLM('gaussian')\n</code></pre> - Use Case: Continuous data with constant variance - Link Function: Identity (<code>\u03bc = X\u03b2</code>) - Parameters: Mean (\u03bc), standard deviation (\u03c3) - Best For: Symmetric, unbounded data (temperatures, stock returns)</p>"},{"location":"api/models/glm/#gamma-distribution","title":"Gamma Distribution","text":"<p><pre><code>glm = GLM('gamma')\n</code></pre> - Use Case: Positive continuous data with right skew - Link Function: Log (<code>log(\u03bc) = X\u03b2</code>) - Parameters: Shape (\u03b1), scale (\u03b2) - Best For: Insurance claims, waiting times, sales amounts</p>"},{"location":"api/models/glm/#inverse-gaussian-distribution","title":"Inverse Gaussian Distribution","text":"<p><pre><code>glm = GLM('inversegaussian')\n</code></pre> - Use Case: Positive data with extreme right tail - Link Function: Log (<code>log(\u03bc) = X\u03b2</code>) - Parameters: Mean (\u03bc), dispersion (\u03bb) - Best For: First passage times, service durations</p>"},{"location":"api/models/glm/#log-normal-distribution","title":"Log-Normal Distribution","text":"<p><pre><code>glm = GLM('lognormal')\n</code></pre> - Use Case: Positive data from multiplicative processes - Link Function: Identity on log-scale - Parameters: Log-mean (\u03bc), log-std (\u03c3) - Best For: Income distributions, growth rates, file sizes</p>"},{"location":"api/models/glm/#quick-start","title":"Quick Start","text":""},{"location":"api/models/glm/#basic-usage","title":"Basic Usage","text":"<pre><code>from drn import GLM\nimport pandas as pd\n\n# Load data\nX = pd.DataFrame({'age': [25, 35, 45], 'income': [30000, 50000, 70000]})\ny = pd.Series([1200, 1800, 2400])\n\n# Fit GLM using statsmodels approach\nglm = GLM('gamma')\nglm.fit(X, y)\n\n# Make predictions\npredictions = glm.predict(X_test)\nmean_pred = predictions.mean\nquantiles = predictions.quantiles([10, 50, 90])\n</code></pre>"},{"location":"api/models/glm/#alternative-gradient-descent-training","title":"Alternative: Gradient Descent Training","text":"<pre><code># Neural network style training\nglm = GLM('gaussian', learning_rate=0.001)\nglm.fit(\n    X_train, y_train,\n    X_val, y_val,\n    grad_descent=True,\n    epochs=1000,\n    batch_size=128,\n    patience=50\n)\n</code></pre>"},{"location":"api/models/glm/#detailed-examples","title":"Detailed Examples","text":""},{"location":"api/models/glm/#insurance-claims-modeling","title":"Insurance Claims Modeling","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom drn import GLM\nfrom drn.metrics import rmse, crps\n\n# Load insurance data\nclaims_data = pd.read_csv('insurance_claims.csv')\nX = claims_data[['age', 'vehicle_age', 'region', 'policy_type']]\ny = claims_data['claim_amount']\n\n# Split data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Train Gamma GLM (ideal for insurance claims)\nglm_gamma = GLM('gamma')\nglm_gamma.fit(X_train, y_train)\n\n# Evaluate predictions\npred_dist = glm_gamma.predict(X_test)\n\n# Point prediction accuracy\nrmse_score = rmse(y_test, pred_dist.mean)\nprint(f\"RMSE: ${rmse_score:.2f}\")\n\n# Distributional accuracy\ngrid = np.linspace(0, y_test.max() * 1.2, 1000)\ncrps_score = crps(y_test, grid, pred_dist.cdf(grid)).mean()\nprint(f\"CRPS: ${crps_score:.2f}\")\n\n# Risk measures\npercentiles = [50, 75, 90, 95, 99]\nrisk_measures = glm_gamma.quantiles(X_test, percentiles)\n\nprint(\"Risk Measures:\")\nfor i, p in enumerate(percentiles):\n    avg_risk = risk_measures[:, i].mean()\n    print(f\"{p}th percentile: ${avg_risk:.2f}\")\n</code></pre>"},{"location":"api/models/glm/#parameter-access-and-interpretation","title":"Parameter Access and Interpretation","text":""},{"location":"api/models/glm/#extracting-fitted-parameters","title":"Extracting Fitted Parameters","text":"<pre><code># After fitting\nglm = GLM('gamma')\nglm.fit(X_train, y_train)\n\n# Access parameters\nprint(\"Model Parameters:\")\nprint(f\"Coefficients: {glm.linear.weight.data}\")\nprint(f\"Intercept: {glm.linear.bias.data}\")\nprint(f\"Dispersion: {glm.dispersion.data}\")\n\n# Interpret coefficients (for Gamma GLM with log link)\nfeature_names = X_train.columns\nfor i, name in enumerate(feature_names):\n    coef = glm.linear.weight[0, i].item()\n    effect = np.exp(coef)  # Multiplicative effect\n    print(f\"{name}: {coef:.4f} (multiplicative effect: {effect:.4f})\")\n</code></pre>"},{"location":"api/models/glm/#statsmodels-equivalence","title":"Statsmodels equivalence","text":"<p>Under the hood, <code>GLM</code> uses <code>statsmodels</code> for statistical fitting. You can make the equivalent GLM directly to access detailed statistical outputs.</p> <pre><code>import statsmodels.api as sm\nfrom statsmodels.genmod.families import Gamma\nimport numpy as np\n\n# For detailed statistical analysis, access underlying statsmodels results\nX_np = X_train.values\ny_np = y_train.values\n\n# Fit using statsmodels directly for statistical inference\nX_sm = sm.add_constant(X_np)\nsm_model = sm.GLM(y_np, X_sm, family=Gamma(link=sm.families.links.Log()))\nsm_results = sm_model.fit()\n\nprint(sm_results.summary())\nprint(f\"AIC: {sm_results.aic:.2f}\")\nprint(f\"BIC: {sm_results.bic:.2f}\")\n</code></pre>"},{"location":"api/models/glm/#advanced-features","title":"Advanced Features","text":""},{"location":"api/models/glm/#integration-with-drn","title":"Integration with DRN","text":"<pre><code>from drn import GLM, DRN\n\n# Train baseline GLM\nbaseline = GLM('gamma')\nbaseline.fit(X_train, y_train)\n\n# Initialize DRN with GLM baseline\ndrn_model = DRN(baseline=baseline).fit(X_train, y_train)\n</code></pre>"},{"location":"api/models/mdn/","title":"MDN - Mixture Density Network","text":"<p>Neural network that models complex distributions as mixtures of simpler components.</p>"},{"location":"api/models/mdn/#class-definition","title":"Class Definition","text":"<p>               Bases: <code>BaseModel</code></p> <p>Mixture density network that can switch between gamma and Gaussian distribution components. The distributional forecasts are mixtures of <code>num_components</code> specified distributions.</p>"},{"location":"api/models/mdn/#drn.models.mdn.MDN-functions","title":"Functions","text":""},{"location":"api/models/mdn/#drn.models.mdn.MDN.__init__","title":"<code>__init__(distribution='gamma', num_hidden_layers=2, num_components=5, hidden_size=100, dropout_rate=0.2, learning_rate=0.001)</code>","text":"<p>Args:     p: the number of features in the model.     num_hidden_layers: the number of hidden layers in the network.     num_components: the number of components in the mixture.     hidden_size: the number of neurons in each hidden layer.     distribution: the type of distribution for the MDN ('gamma' or 'gaussian').</p>"},{"location":"api/models/mdn/#drn.models.mdn.MDN.forward","title":"<code>forward(x)</code>","text":"<p>Calculate the parameters of the mixture components. Args:     x: the input features (shape: (n, p)) Returns:     A list containing the mixture weights, and distribution-specific parameters.</p>"},{"location":"api/models/mdn/#drn.models.mdn.MDN.mean","title":"<code>mean(x)</code>","text":"<p>Calculate the predicted means for the given observations, depending on the mixture distribution. Args:     x: the input features (shape: (n, p)) Returns:     the predicted means (shape: (n,))</p>"},{"location":"api/models/mdn/#overview","title":"Overview","text":"<p>MDN (Mixture Density Network) is ideal for: - Multi-modal distributions - Data with multiple peaks - Complex relationships - Non-linear feature-target mappings - Uncertainty quantification - Rich distributional representations - Flexible modeling - Adaptive number of mixture components</p>"},{"location":"api/models/mdn/#architecture","title":"Architecture","text":"<pre><code>graph LR\n    A[Input Features] --&gt; B[Neural Network]\n    B --&gt; C[Mixing Weights]\n    B --&gt; D[Component Means]\n    B --&gt; E[Component Stds]\n\n    C --&gt; F[Mixture Distribution]\n    D --&gt; F\n    E --&gt; F\n\n    F --&gt; G[Predictions]</code></pre>"},{"location":"api/models/mdn/#quick-example","title":"Quick Example","text":"<pre><code>from drn.models import MDN\nimport torch\n\n# Initialize MDN with 3 mixture components\nmdn_model = MDN(\n    input_dim=8,\n    num_components=3,\n    hidden_size=128,\n    num_hidden_layers=2\n)\n\n# Train on complex multimodal data\nmdn_model.fit(X_train, y_train, epochs=150)\n\n# Generate predictions\npredictions = mdn_model.predict(X_test)\n\n# Access mixture properties\nmixing_weights = predictions.mixture_distribution.probs\ncomponent_means = predictions.mixture_distribution.component_distribution.mean\n</code></pre>"},{"location":"getting-started/advanced-usage/","title":"Advanced Usage Guide","text":"<p>This guide covers advanced DRN usage for users who want direct control over PyTorch tensors and the training process. Use this when you need custom training loops or integration with existing PyTorch codebases.</p>"},{"location":"getting-started/advanced-usage/#when-to-use-advanced-mode","title":"When to Use Advanced Mode","text":"<p>Use advanced mode when you need:</p> <ul> <li>Custom training loops with specific optimization strategies</li> <li>Manual tensor management for performance optimization</li> <li>Integration with existing PyTorch codebases </li> <li>Fine-grained control over model training</li> <li>Custom loss functions beyond the provided ones</li> </ul>"},{"location":"getting-started/advanced-usage/#core-concepts","title":"Core Concepts","text":""},{"location":"getting-started/advanced-usage/#tensor-first-workflow","title":"Tensor-First Workflow","text":"<pre><code>import torch\nfrom drn import GLM, DRN, train\nimport numpy as np\n\n# Manual tensor conversion (based on test patterns)\ndef generate_tensor_data(n=1000, seed=1):\n    \"\"\"Generate tensor data similar to test_fit_models_synthetic.py\"\"\"\n    rng = np.random.default_rng(seed)\n    x_all = rng.random(size=(n, 4))\n    epsilon = rng.normal(0, 0.2, n)\n\n    means = np.exp(\n        0\n        - 0.5 * x_all[:, 0]\n        + 0.5 * x_all[:, 1]\n        + np.sin(np.pi * x_all[:, 0])\n        - np.sin(np.pi * np.log(x_all[:, 2] + 1))\n        + np.cos(x_all[:, 1] * x_all[:, 2])\n    ) + np.cos(x_all[:, 1])\n\n    y_all = means + epsilon**2\n\n    # Convert to tensors\n    X_tensor = torch.tensor(x_all, dtype=torch.float32)\n    y_tensor = torch.tensor(y_all, dtype=torch.float32)\n\n    return X_tensor, y_tensor\n\n# Generate tensor data\nX_train, y_train = generate_tensor_data(800, seed=1)\nX_val, y_val = generate_tensor_data(200, seed=2)\n\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"y_train range: [{y_train.min():.3f}, {y_train.max():.3f}]\")\n</code></pre>"},{"location":"getting-started/advanced-usage/#using-the-train-function-directly","title":"Using the <code>train</code> Function Directly","text":"<pre><code># Create PyTorch datasets (as in tests)\ntrain_dataset = torch.utils.data.TensorDataset(X_train, y_train)\nval_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n\n# Train GLM using the train function (from test_fit_models_synthetic.py)\ntorch.manual_seed(1)\nglm = GLM(\"gamma\")\ntrain(glm, train_dataset, val_dataset, epochs=5)\n\nprint(\"\u2713 GLM training completed using train() function\")\n</code></pre>"},{"location":"getting-started/advanced-usage/#manual-dispersion-updates","title":"Manual Dispersion Updates","text":"<pre><code># Update dispersion after training (from tests)\nglm.update_dispersion(X_train, y_train)\nprint(f\"Updated dispersion: {glm.dispersion.item():.4f}\")\n</code></pre>"},{"location":"getting-started/advanced-usage/#drn-training-with-custom-parameters","title":"DRN Training with Custom Parameters","text":"<p>Based on test patterns, here's how to train DRN with manual control:</p> <pre><code>from drn.models import drn_cutpoints\n\n# Create cutpoints (pattern from tests)\ncutpoints = drn_cutpoints(\n    c_0=y_train.min().item() * 0.9,\n    c_K=y_train.max().item() * 1.1,\n    proportion=0.1,\n    y=y_train.numpy(),\n    min_obs=10\n)\n\nprint(f\"Generated {len(cutpoints)} cutpoints\")\n\n# Create DRN with baseline\ntorch.manual_seed(2)  # For reproducibility (as in tests)\ndrn = DRN(glm, cutpoints, num_hidden_layers=2, hidden_size=100)\n\n# Train using the train function\ntrain(drn, train_dataset, val_dataset, epochs=5, lr=0.001)\n\nprint(\"\u2713 DRN training completed\")\n</code></pre>"},{"location":"getting-started/advanced-usage/#model-evaluation-with-crps","title":"Model Evaluation with CRPS","text":"<p>From the test suite, here's how to properly evaluate models:</p> <pre><code>from drn.metrics import crps\n\ndef evaluate_model_crps(model, X_test, y_test, grid_size=1000):\n    \"\"\"Evaluate model using CRPS (from test patterns).\"\"\"\n\n    # Generate grid for CRPS calculation\n    grid = torch.linspace(0, y_test.max().item() * 1.1, grid_size).unsqueeze(-1)\n\n    # Get model predictions\n    dists = model.predict(X_test)\n    cdfs = dists.cdf(grid)\n\n    # Calculate CRPS\n    grid = grid.squeeze()\n    crps_scores = crps(y_test, grid, cdfs)\n\n    return crps_scores.mean()\n\n# Evaluate both models\nX_test, y_test = generate_tensor_data(200, seed=3)\n\nglm_crps = evaluate_model_crps(glm, X_test, y_test)\ndrn_crps = evaluate_model_crps(drn, X_test, y_test)\n\nprint(f\"GLM CRPS: {glm_crps:.4f}\")\nprint(f\"DRN CRPS: {drn_crps:.4f}\")\nprint(f\"CRPS improvement: {((glm_crps - drn_crps) / glm_crps * 100):.1f}%\")\n</code></pre>"},{"location":"getting-started/advanced-usage/#cann-model-training","title":"CANN Model Training","text":"<p>Based on the test patterns for CANN:</p> <pre><code>from drn.models import CANN\n\n# Train CANN (from test_fit_models_synthetic.py pattern)\ntorch.manual_seed(2)\nbaseline_for_cann = GLM(\"gamma\")\ntrain(baseline_for_cann, train_dataset, val_dataset, epochs=2)\n\ncann = CANN(baseline_for_cann, num_hidden_layers=2, hidden_size=100)\ntrain(cann, train_dataset, val_dataset, epochs=2)\n\nprint(\"\u2713 CANN training completed\")\n\n# Evaluate CANN\ncann_crps = evaluate_model_crps(cann, X_test, y_test)\nprint(f\"CANN CRPS: {cann_crps:.4f}\")\n</code></pre>"},{"location":"getting-started/advanced-usage/#device-management","title":"Device Management","text":"<p>For GPU usage (based on test patterns):</p> <pre><code># Check device availability (from test patterns)\nif torch.cuda.is_available():\n    device = torch.device(\"cuda:0\")\n    print(f\"Using GPU: {device}\")\nelse:\n    device = torch.device(\"cpu\")\n    print(f\"Using CPU: {device}\")\n\n# Move data to device\nX_train = X_train.to(device)\ny_train = y_train.to(device)\nX_val = X_val.to(device)\ny_val = y_val.to(device)\n\n# Move model to device\nglm = glm.to(device)\n\nprint(f\"\u2713 Data and model moved to {device}\")\n</code></pre>"},{"location":"getting-started/advanced-usage/#working-with-different-model-types","title":"Working with Different Model Types","text":""},{"location":"getting-started/advanced-usage/#glm-with-different-distributions","title":"GLM with Different Distributions","text":"<pre><code># Test different GLM distributions (pattern from test_glm_distributions.py)\ndistributions = ['gaussian', 'gamma']\n\nfor dist_name in distributions:\n    print(f\"\\nTraining GLM with {dist_name} distribution:\")\n\n    # Create and train model\n    glm_dist = GLM(dist_name)\n    train(glm_dist, train_dataset, val_dataset, epochs=3)\n\n    # Evaluate\n    crps_score = evaluate_model_crps(glm_dist, X_test, y_test)\n    print(f\"{dist_name} GLM CRPS: {crps_score:.4f}\")\n</code></pre>"},{"location":"getting-started/advanced-usage/#quantile-evaluation","title":"Quantile Evaluation","text":"<p>Testing quantile functionality (from test patterns):</p> <pre><code>from drn.utils import binary_search_icdf\n\n# Test quantile calculation\ntest_percentiles = [10, 50, 90]\nquantiles = glm.quantiles(X_test[:5], test_percentiles)\n\nprint(f\"Quantiles shape: {quantiles.shape}\")\nprint(f\"Test percentiles: {test_percentiles}\")\nprint(f\"Quantile values for first sample: {quantiles[0]}\")\n</code></pre>"},{"location":"getting-started/advanced-usage/#model-checkpointing","title":"Model Checkpointing","text":"<p>Basic model saving/loading (minimal example):</p> <pre><code># Save model state\ntorch.save(glm.state_dict(), 'glm_model.pth')\ntorch.save(drn.state_dict(), 'drn_model.pth')\n\n# Load model state\nglm_loaded = GLM(\"gamma\")\nglm_loaded.load_state_dict(torch.load('glm_model.pth'))\nglm_loaded.eval()\n\nprint(\"\u2713 Model checkpointing completed\")\n</code></pre>"},{"location":"getting-started/advanced-usage/#advanced-training-configuration","title":"Advanced Training Configuration","text":"<p>Using PyTorch Lightning trainer options (from test patterns):</p> <pre><code># Advanced training with specific parameters\ntrain(\n    drn,\n    train_dataset,\n    val_dataset,\n    epochs=10,\n    batch_size=64,\n    lr=0.001,\n    patience=5,\n    # Additional trainer arguments\n    accelerator='cpu',  # or 'gpu' if available\n    devices=1,\n    enable_progress_bar=True\n)\n</code></pre>"},{"location":"getting-started/advanced-usage/#performance-optimization","title":"Performance Optimization","text":""},{"location":"getting-started/advanced-usage/#batch-size-optimization","title":"Batch Size Optimization","text":"<pre><code># Test different batch sizes for performance\nbatch_sizes = [32, 64, 128]\n\nfor batch_size in batch_sizes:\n    print(f\"Testing batch size: {batch_size}\")\n\n    # Create data loaders with different batch sizes\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, \n        batch_size=batch_size, \n        shuffle=True\n    )\n\n    # Time training step\n    import time\n    model = GLM(\"gamma\")\n    start_time = time.time()\n    train(model, train_dataset, val_dataset, epochs=1)\n    training_time = time.time() - start_time\n\n    print(f\"Training time with batch_size {batch_size}: {training_time:.2f}s\")\n</code></pre>"},{"location":"getting-started/advanced-usage/#integration-with-existing-pytorch-code","title":"Integration with Existing PyTorch Code","text":"<p>If you have existing PyTorch training loops:</p> <pre><code>import torch.nn as nn\nimport torch.optim as optim\n\n# Manual training loop (advanced users)\ndef custom_training_loop(model, train_dataset, epochs=5):\n    \"\"\"Custom training loop for advanced users.\"\"\"\n\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            loss = model.loss(data, target)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n        print(f'Epoch {epoch+1}/{epochs}, Average Loss: {total_loss/len(train_loader):.4f}')\n\n    model.eval()\n\n# Use custom training loop\ncustom_model = GLM(\"gamma\")\ncustom_training_loop(custom_model, train_dataset, epochs=3)\n</code></pre>"},{"location":"getting-started/advanced-usage/#key-differences-from-simple-usage","title":"Key Differences from Simple Usage","text":"<p>Data Handling</p> <ul> <li>Manual tensor conversion and device management</li> <li>Explicit DataLoader creation</li> <li>Direct control over batching and shuffling</li> </ul> <p>Training Control</p> <ul> <li>Use <code>train()</code> function instead of <code>.fit()</code></li> <li>Manual epoch and learning rate management</li> <li>Custom training loops possible</li> </ul> <p>Evaluation</p> <ul> <li>Manual CRPS calculation with grid generation</li> <li>Direct access to distribution objects</li> <li>Custom metric implementation</li> </ul>"},{"location":"getting-started/advanced-usage/#when-to-use-each-approach","title":"When to Use Each Approach","text":"<p>Use Simple Usage (pandas/numpy) when:</p> <ul> <li>Prototyping and experimenting</li> <li>Standard workflows are sufficient</li> <li>Working with mixed data types</li> <li>Want scikit-learn-like interface</li> </ul> <p>Use Advanced Usage (tensors) when:</p> <ul> <li>Need custom training loops</li> <li>Integrating with existing PyTorch code</li> <li>Performance optimization required</li> <li>Custom loss functions needed</li> </ul>"},{"location":"getting-started/advanced-usage/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Complete technical documentation</li> <li>Training - Training function details</li> <li>Quick Start - Compare with pandas approach</li> </ul>"},{"location":"getting-started/concepts/","title":"Basic Concepts","text":"<p>This page introduces the fundamental concepts behind Distributional Refinement Networks (DRN) and distributional regression modeling.</p>"},{"location":"getting-started/concepts/#what-is-distributional-regression","title":"What is Distributional Regression?","text":"<p>Traditional regression models predict a single value (the mean) for each input. Distributional regression goes beyond this by modeling the entire conditional distribution of the response variable given the features.</p>"},{"location":"getting-started/concepts/#traditional-regression","title":"Traditional Regression","text":"<pre><code>X \u2192 \u03bc (mean only)\n</code></pre>"},{"location":"getting-started/concepts/#distributional-regression","title":"Distributional Regression","text":"<pre><code>X \u2192 P(Y|X) (entire distribution)\n</code></pre> <p>This allows us to predict: - Mean: Expected value - Quantiles: Risk measures (e.g., 95th percentile) - Density: Full probability distribution - Uncertainty: Prediction intervals and confidence</p>"},{"location":"getting-started/concepts/#the-drn-architecture","title":"The DRN Architecture","text":""},{"location":"getting-started/concepts/#core-philosophy","title":"Core Philosophy","text":"<p>DRN addresses three key challenges in actuarial and statistical modeling:</p> <ol> <li>Flexible Covariate Effects: Features should be able to affect different parts of the distribution differently</li> <li>Interpretability vs. Performance: Maintain model transparency while leveraging ML advances</li> <li>Distributional Focus: Model the entire distribution, not just the mean</li> </ol>"},{"location":"getting-started/concepts/#two-stage-approach","title":"Two-Stage Approach","text":"<pre><code>graph LR\n    A[Input Features X] --&gt; B[Baseline Model]\n    A --&gt; C[Neural Network]\n    B --&gt; D[Baseline Distribution]\n    C --&gt; E[Refinement Factors]\n    D --&gt; F[DRN Distribution]\n    E --&gt; F\n    F --&gt; G[Predictions]</code></pre>"},{"location":"getting-started/concepts/#stage-1-baseline-model","title":"Stage 1: Baseline Model","text":"<ul> <li>Usually a Generalized Linear Model (GLM)</li> <li>Provides interpretable foundation</li> <li>Captures main relationships in data</li> <li>Well-understood statistical properties</li> </ul>"},{"location":"getting-started/concepts/#stage-2-neural-refinement","title":"Stage 2: Neural Refinement","text":"<ul> <li>Deep neural network refines the baseline</li> <li>Operates on discretized regions (cutpoints)</li> <li>Constrained by regularization terms</li> <li>Maintains distributional coherence</li> </ul>"},{"location":"getting-started/concepts/#mathematical-framework","title":"Mathematical Framework","text":"<p>For a given input x, the DRN produces a distribution where:</p> <ul> <li>Baseline: <code>P\u2080(y|x)</code> from GLM or other interpretable model</li> <li>Refinement: Neural network produces adjustment factors</li> <li>Final Distribution: Refined distribution that respects baseline structure</li> </ul> <p>The refinement is controlled by three key regularization terms:</p> <ol> <li>KL Divergence (<code>kl_alpha</code>): Controls deviation from baseline distribution</li> <li>Roughness Penalty (<code>dv_alpha</code>): Ensures smooth density functions</li> <li>Mean Penalty (<code>mean_alpha</code>): Controls deviation from baseline mean</li> </ol>"},{"location":"getting-started/concepts/#key-components","title":"Key Components","text":""},{"location":"getting-started/concepts/#1-cutpoints-system","title":"1. Cutpoints System","text":"<p>DRN operates on discretized regions of the response variable:</p> <pre><code>[c\u2080, c\u2081) [c\u2081, c\u2082) ... [c\u2096\u208b\u2081, c\u2096]\n</code></pre> <ul> <li>c\u2080: Lower bound of refinement region</li> <li>c\u2096: Upper bound of refinement region  </li> <li>Number of intervals: Determined by cutpoints-to-observation ratio <code>p</code></li> </ul> <p>Why discretize? - Makes neural network training stable - Allows flexible density shapes - Enables efficient computation - Maintains probabilistic coherence</p>"},{"location":"getting-started/concepts/#2-distribution-objects","title":"2. Distribution Objects","text":"<p>All models return distribution objects with methods:</p> <pre><code>dist = model.predict(X)\n\n# Point estimates\nmean = dist.mean                    # Expected value\nmode = dist.mode                    # Most likely value\n\n# Distributional properties  \npdf = dist.density(y_grid)          # Probability density\ncdf = dist.cdf(y_grid)              # Cumulative distribution\nquantiles = dist.quantiles([5, 95]) # Risk measures\n\n# Evaluation\nlog_prob = dist.log_prob(y_true)    # Log-likelihood\n</code></pre>"},{"location":"getting-started/concepts/#3-training-framework","title":"3. Training Framework","text":"<p>DRN training follows a specific pattern:</p> <pre><code># 1. Train baseline\nbaseline = GLM('gaussian').fit(X, y)\n\n# 2. Define refinement region\ncutpoints = drn_cutpoints(c_0, c_K, p, y, min_obs)\n\n# 3. Initialize DRN\ndrn = DRN(baseline, cutpoints, hidden_size=128)\n\n# 4. Train with custom loss\ntrain(drn, drn_loss, train_data, val_data)\n</code></pre>"},{"location":"getting-started/concepts/#model-types-in-drn","title":"Model Types in DRN","text":""},{"location":"getting-started/concepts/#baseline-models","title":"Baseline Models","text":""},{"location":"getting-started/concepts/#glm-generalized-linear-models","title":"GLM (Generalized Linear Models)","text":"<ul> <li>Gaussian: Normal distribution with linear mean</li> <li>Gamma: Gamma distribution for positive responses</li> <li>Interpretable coefficients</li> <li>Well-established statistical theory</li> </ul>"},{"location":"getting-started/concepts/#constant-model","title":"Constant Model","text":"<ul> <li>Simple baseline predicting constant distribution</li> <li>Useful for ablation studies</li> <li>Minimal computational overhead</li> </ul>"},{"location":"getting-started/concepts/#advanced-models","title":"Advanced Models","text":""},{"location":"getting-started/concepts/#drn-distributional-refinement-network","title":"DRN (Distributional Refinement Network)","text":"<ul> <li>Main model of the package</li> <li>Combines interpretable baseline + neural refinement</li> <li>Flexible distribution shapes</li> <li>Controlled regularization</li> </ul>"},{"location":"getting-started/concepts/#cann-combined-actuarial-neural-network","title":"CANN (Combined Actuarial Neural Network)","text":"<ul> <li>Actuarial-focused architecture</li> <li>Separate networks for different distributional parameters</li> <li>Industry-standard approach</li> </ul>"},{"location":"getting-started/concepts/#mdn-mixture-density-network","title":"MDN (Mixture Density Network)","text":"<ul> <li>Models multimodal distributions</li> <li>Mixture of simple distributions</li> <li>Good for complex, multi-peaked data</li> </ul>"},{"location":"getting-started/concepts/#ddr-deep-distribution-regression","title":"DDR (Deep Distribution Regression)","text":"<ul> <li>Pure neural approach to distributional regression</li> <li>No baseline constraint</li> <li>Maximum flexibility</li> </ul>"},{"location":"getting-started/concepts/#regularization-and-control","title":"Regularization and Control","text":""},{"location":"getting-started/concepts/#kl-divergence-control-kl_alpha","title":"KL Divergence Control (<code>kl_alpha</code>)","text":"<p>Controls how much the final distribution can deviate from the baseline:</p> <ul> <li>Small values (1e-5 to 1e-4): Stay close to baseline</li> <li>Larger values: Allow more deviation</li> <li>Direction: Forward or reverse KL divergence</li> </ul>"},{"location":"getting-started/concepts/#roughness-penalty-dv_alpha","title":"Roughness Penalty (<code>dv_alpha</code>)","text":"<p>Ensures smooth density functions:</p> <ul> <li>Larger values: Smoother densities</li> <li>Smaller values: Allow more complex shapes</li> <li>Balance: Trade-off between flexibility and stability</li> </ul>"},{"location":"getting-started/concepts/#mean-penalty-mean_alpha","title":"Mean Penalty (<code>mean_alpha</code>)","text":"<p>Controls deviation of predicted mean from baseline:</p> <ul> <li>Zero: No constraint on mean</li> <li>Small values (1e-5 to 1e-4): Gentle constraint</li> <li>Larger values: Force mean to stay close to baseline</li> </ul>"},{"location":"getting-started/concepts/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"getting-started/concepts/#distribution-aware-metrics","title":"Distribution-Aware Metrics","text":"<p>Unlike traditional regression, distributional models need special evaluation:</p>"},{"location":"getting-started/concepts/#crps-continuous-ranked-probability-score","title":"CRPS (Continuous Ranked Probability Score)","text":"<ul> <li>Measures difference between predicted CDF and observed outcome</li> <li>Lower is better</li> <li>Rewards both accuracy and calibration</li> </ul>"},{"location":"getting-started/concepts/#quantile-loss","title":"Quantile Loss","text":"<ul> <li>Evaluates specific quantile predictions</li> <li>Asymmetric loss function</li> <li>Important for risk management</li> </ul>"},{"location":"getting-started/concepts/#log-likelihood-nll","title":"Log-Likelihood (NLL)","text":"<ul> <li>Measures how well model assigns probability to observed outcomes</li> <li>Higher likelihood = better fit</li> <li>Can overfit if not regularized</li> </ul>"},{"location":"getting-started/concepts/#traditional-metrics","title":"Traditional Metrics","text":"<ul> <li>RMSE: Still useful for mean predictions</li> <li>MAE: Less sensitive to outliers</li> <li>R\u00b2: Explained variance (for means only)</li> </ul>"},{"location":"getting-started/concepts/#next-steps","title":"Next Steps","text":"<p>Now that you understand the concepts, you can:</p> <ol> <li>Try the Quick Start Guide - Hands-on experience</li> <li>Explore Advanced Usage - Step-by-step examples</li> </ol>"},{"location":"getting-started/concepts/#further-reading","title":"Further Reading","text":"<ul> <li>DRN Paper</li> <li>CANN Paper</li> <li>DeepGLM Paper</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start Guide","text":"<p>This guide shows how to use DRN with pandas and numpy. The library handles all tensor conversions internally, so you can work with familiar data structures.</p>"},{"location":"getting-started/quickstart/#installation","title":"Installation","text":"<p>Install DRN using pip:</p> <pre><code>pip install drn\n</code></pre> <p>DRN requires Python 3.8+ and will automatically install PyTorch, pandas, numpy, scikit-learn, and other dependencies.</p>"},{"location":"getting-started/quickstart/#1-basic-example","title":"1. Basic Example","text":"<pre><code>from drn import GLM, DRN\nimport pandas as pd\nimport numpy as np\n\n# Load your data (pandas/numpy)\nX_train = pd.DataFrame({\n    'age': [25, 35, 45, 55, 30],\n    'income': [30000, 50000, 70000, 90000, 45000]\n})\ny_train = pd.Series([1200, 1800, 2400, 3000, 1500])\n\n# 1. Train baseline model (GLM handles data conversion internally)\nbaseline = GLM('gamma')  # Good for positive targets like insurance claims\nbaseline.fit(X_train, y_train)\n\n# 2. Train DRN (automatically handles data conversion)\ndrn_model = DRN(baseline)\ndrn_model.fit(X_train, y_train)\n\n# 3. Make predictions (returns distribution objects)\npredictions = drn_model.predict(X_train)\nmean_predictions = predictions.mean\nquantiles = predictions.quantiles([10, 50, 90])\n\nprint(f\"Mean predictions: {mean_predictions}\")\nprint(f\"Quantiles (10th, 50th, 90th): {quantiles}\")\n</code></pre>"},{"location":"getting-started/quickstart/#2-complete-example-with-realistic-data","title":"2. Complete Example with Realistic Data","text":"<p>Let's create a more complete example using tested patterns from the codebase:</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom drn import GLM, DRN\nfrom drn.metrics import rmse\n\ndef generate_realistic_data(n=1000, seed=42):\n    \"\"\"Generate data similar to the tested synthetic dataset.\"\"\"\n    np.random.seed(seed)\n\n    # Create features (based on actual test patterns)\n    X = np.random.random(size=(n, 4))\n\n    # Create complex target relationship (from test_fit_models_synthetic.py)\n    means = np.exp(\n        0\n        - 0.5 * X[:, 0]\n        + 0.5 * X[:, 1]\n        + np.sin(np.pi * X[:, 0])\n        - np.sin(np.pi * np.log(X[:, 2] + 1))\n        + np.cos(X[:, 1] * X[:, 2])\n    ) + np.cos(X[:, 1])\n\n    # Add noise\n    epsilon = np.random.normal(0, 0.2, n)\n    y = means + epsilon**2\n\n    # Convert to pandas\n    X_df = pd.DataFrame(X, columns=['feature_0', 'feature_1', 'feature_2', 'feature_3'])\n    y_series = pd.Series(y, name='target')\n\n    return X_df, y_series\n\n# Generate synthetic data\nX, y = generate_realistic_data(n=1000)\nprint(f\"Dataset shape: {X.shape}\")\nprint(f\"Target range: [{y.min():.2f}, {y.max():.2f}]\")\n</code></pre>"},{"location":"getting-started/quickstart/#3-split-data-simple-approach","title":"3. Split Data (Simple Approach)","text":"<pre><code># Simple train/test split (pandas stays as pandas)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(f\"Training set: {X_train.shape[0]} samples\")\nprint(f\"Test set: {X_test.shape[0]} samples\")\n</code></pre>"},{"location":"getting-started/quickstart/#4-train-baseline-glm","title":"4. Train Baseline GLM","text":"<pre><code># Train baseline GLM (uses pandas/numpy internally)\nbaseline = GLM('gamma')  # Gamma distribution for positive targets\nbaseline.fit(X_train, y_train)\n\n# Check baseline performance\nbaseline_pred = baseline.predict(X_test)\nbaseline_rmse = rmse(y_test, baseline_pred.mean)\nprint(f\"Baseline RMSE: {baseline_rmse:.4f}\")\n</code></pre>"},{"location":"getting-started/quickstart/#5-train-drn","title":"5. Train DRN","text":"<pre><code># Create DRN (automatically determines cutpoints)\ndrn_model = DRN(baseline)\ndrn_model.fit(X_train, y_train)\n\n# Check DRN performance\ndrn_pred = drn_model.predict(X_test)\ndrn_rmse = rmse(y_test, drn_pred.mean)\n\nprint(f\"DRN RMSE: {drn_rmse:.4f}\")\nprint(f\"Improvement: {((baseline_rmse - drn_rmse) / baseline_rmse * 100):.1f}%\")\n</code></pre>"},{"location":"getting-started/quickstart/#6-explore-predictions","title":"6. Explore Predictions","text":"<pre><code># Get distributional properties\nsingle_prediction = drn_model.predict(X_test.iloc[:1])\n\nprint(f\"Mean prediction: {single_prediction.mean.item():.3f}\")\nprint(f\"True value: {y_test.iloc[0]:.3f}\")\n\n# Get risk measures\nrisk_quantiles = drn_model.quantiles(X_test.iloc[:1], [5, 25, 50, 75, 95])\nprint(f\"5th-95th percentile range: [{risk_quantiles[0][0]:.3f}, {risk_quantiles[0][-1]:.3f}]\")\n\n# Compare multiple models\nmodels = {'Baseline': baseline, 'DRN': drn_model}\nfor name, model in models.items():\n    pred = model.predict(X_test)\n    test_rmse = rmse(y_test, pred.mean)\n    print(f\"{name} RMSE: {test_rmse:.4f}\")\n</code></pre>"},{"location":"getting-started/quickstart/#7-working-with-different-data-types","title":"7. Working with Different Data Types","text":""},{"location":"getting-started/quickstart/#mixed-data-types-pandas","title":"Mixed Data Types (Pandas)","text":"<pre><code># DRN handles mixed data automatically\nmixed_data = pd.DataFrame({\n    'age': [25, 35, 45, 55],\n    'income': [30000, 50000, 70000, 90000],\n    'region': ['North', 'South', 'East', 'West'],  # Categorical\n    'policy_type': ['Basic', 'Premium', 'Basic', 'Premium']  # Categorical\n})\n\n# DRN will handle categorical encoding internally\nglm_mixed = GLM('gamma')\n# Note: For categorical data, you may want to use preprocessing utilities\n</code></pre>"},{"location":"getting-started/quickstart/#numpy-arrays","title":"NumPy Arrays","text":"<pre><code># DRN also works with pure numpy\nX_numpy = X_train.values\ny_numpy = y_train.values\n\nglm_numpy = GLM('gamma')\nglm_numpy.fit(X_numpy, y_numpy)\n</code></pre>"}]}